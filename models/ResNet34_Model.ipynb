{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "749de63d",
      "metadata": {
        "id": "749de63d"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import models\n",
        "import torchvision.transforms.v2 as transforms\n",
        "from PIL import Image\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "3160cd9a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3160cd9a",
        "outputId": "f79b4718-7d92-4e64-d2a9-280ee1d4cd9c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "torch.backends.cudnn.benchmark = True\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7cf044bb",
      "metadata": {
        "id": "7cf044bb"
      },
      "outputs": [],
      "source": [
        "class FruitVegDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.images = []\n",
        "        self.labels = []\n",
        "        self.class_to_idx = {}\n",
        "\n",
        "        print(f\"Initializing dataset from directory: {root_dir}\")\n",
        "        if not os.path.exists(root_dir):\n",
        "            print(f\"ERROR: Directory {root_dir} does not exist\")\n",
        "            return\n",
        "\n",
        "        class_folders = [d for d in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, d))]\n",
        "        print(f\"Found {len(class_folders)} class folders: {class_folders}\")\n",
        "\n",
        "        self.class_to_idx = {class_name: idx for idx, class_name in enumerate(sorted(class_folders))}\n",
        "\n",
        "        for class_name in class_folders:\n",
        "            class_dir = os.path.join(root_dir, class_name)\n",
        "            class_idx = self.class_to_idx[class_name]\n",
        "\n",
        "            for img_name in os.listdir(class_dir):\n",
        "                if img_name.endswith('.jpg') or img_name.endswith('.jpeg') or img_name.endswith('.png'):\n",
        "                    img_path = os.path.join(class_dir, img_name)\n",
        "                    self.images.append(img_path)\n",
        "                    self.labels.append(class_idx)\n",
        "\n",
        "        self.idx_to_class = {idx: class_name for class_name, idx in self.class_to_idx.items()}\n",
        "        print(f\"Dataset created with {len(self.images)} images\")\n",
        "        print(f\"Found {len(self.class_to_idx)} classes: {', '.join(self.class_to_idx.keys())}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.images[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        try:\n",
        "            img = Image.open(img_path).convert('RGB')\n",
        "\n",
        "            if self.transform:\n",
        "                img = self.transform(img)\n",
        "\n",
        "            return img, label\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading image {img_path}: {e}\")\n",
        "            return label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48fe3d2f",
      "metadata": {
        "id": "48fe3d2f"
      },
      "outputs": [],
      "source": [
        "class FruitVegClassifier(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(FruitVegClassifier, self).__init__()\n",
        "\n",
        "        self.backbone = models.resnet34(weights=models.ResNet34_Weights.DEFAULT)\n",
        "\n",
        "        if hasattr(self.backbone, 'fc'):\n",
        "            num_ftrs = self.backbone.fc.in_features\n",
        "            self.backbone.fc = nn.Sequential(\n",
        "                nn.Linear(num_ftrs, 512),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.Dropout(0.3),\n",
        "                nn.Linear(512, num_classes)\n",
        "            )\n",
        "        elif hasattr(self.backbone, 'classifier'):\n",
        "            num_ftrs = self.backbone.classifier[1].in_features\n",
        "            self.backbone.classifier = nn.Sequential(\n",
        "                nn.Dropout(p=0.3, inplace=True),\n",
        "                nn.Linear(num_ftrs, 512),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.Dropout(0.3),\n",
        "                nn.Linear(512, num_classes)\n",
        "            )\n",
        "        self.extra_compute = False\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.backbone(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8e9e96f",
      "metadata": {
        "id": "f8e9e96f"
      },
      "outputs": [],
      "source": [
        "def train_model(model, train_loader, val_loader, criterion, optimizer,\n",
        "                         num_epochs=10, device='cuda', patience=5,\n",
        "                         checkpoint_path='best_model.pth'):\n",
        "    best_val_accuracy = 0.0\n",
        "    no_improve_epochs = 0\n",
        "\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, mode='max', factor=0.5, patience=2\n",
        "    )\n",
        "\n",
        "    scaler = torch.amp.GradScaler() if device == 'cuda' else None\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # Training\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "\n",
        "        print(f\"VRAM Usage: {torch.cuda.memory_allocated() / 1e9:.2f} GB / {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "\n",
        "        for inputs, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Training\"):\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "            # Most of the below were just forced configurations/computations\n",
        "            # to force some workload on my GPU (suggested online)\n",
        "            if scaler:\n",
        "                with torch.amp.autocast():\n",
        "                    inputs = inputs + 0.001 * torch.sin(inputs * 10) \n",
        "\n",
        "                    outputs = model(inputs)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                scaler.scale(loss).backward()\n",
        "                torch.cuda.synchronize()\n",
        "                scaler.step(optimizer)\n",
        "                scaler.update()\n",
        "            else:\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "            if torch.cuda.is_available() and torch.cuda.memory_allocated() > 5e9:  # 5 GB threshold\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "        epoch_loss = running_loss / len(train_loader.dataset)\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Validation\"):\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                val_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "        val_accuracy = correct / total\n",
        "        avg_val_loss = val_loss / len(val_loader.dataset)\n",
        "\n",
        "        scheduler.step(val_accuracy)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}:\")\n",
        "        print(f\"  Train Loss: {epoch_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
        "        print(f\"  Val Accuracy: {val_accuracy:.4f}\")\n",
        "\n",
        "        if val_accuracy > best_val_accuracy:\n",
        "            best_val_accuracy = val_accuracy\n",
        "            no_improve_epochs = 0\n",
        "\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'val_accuracy': val_accuracy,\n",
        "                'class_mapping': train_dataset.class_to_idx\n",
        "            }, checkpoint_path)\n",
        "\n",
        "            print(f\"  Saved new best model with accuracy: {val_accuracy:.4f}\")\n",
        "        else:\n",
        "            no_improve_epochs += 1\n",
        "            print(f\"  No improvement for {no_improve_epochs} epochs. Best accuracy: {best_val_accuracy:.4f}\")\n",
        "\n",
        "        if no_improve_epochs >= patience:\n",
        "            print(f\"Early stopping triggered after {epoch+1} epochs\")\n",
        "            break\n",
        "\n",
        "    checkpoint = torch.load(checkpoint_path)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb0c237d",
      "metadata": {
        "id": "fb0c237d"
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "    train_dir = 'Train'\n",
        "    val_dir = 'val' \n",
        "\n",
        "    # Data transforms\n",
        "    train_transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.RandomCrop(224),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomRotation(15),\n",
        "        transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n",
        "        transforms.ToImage(),\n",
        "        transforms.ToDtype(torch.float32, scale=True),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    val_transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToImage(),\n",
        "        transforms.ToDtype(torch.float32, scale=True),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    global train_dataset\n",
        "    train_dataset = FruitVegDataset(\n",
        "        root_dir=train_dir,\n",
        "        transform=train_transform\n",
        "    )\n",
        "\n",
        "    val_dataset = FruitVegDataset(\n",
        "        root_dir=val_dir,\n",
        "        transform=val_transform\n",
        "    )\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=32,\n",
        "        shuffle=True,\n",
        "        num_workers=4,\n",
        "        pin_memory=True,\n",
        "        prefetch_factor=2\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=32,\n",
        "        shuffle=False,\n",
        "        num_workers=4,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    # Create model\n",
        "    num_classes = len(train_dataset.class_to_idx)\n",
        "    model = FruitVegClassifier(num_classes)\n",
        "\n",
        "    # Initially added this as True to try to \"force\" my system's \n",
        "    # GPU to get to work, but that didn't work.\n",
        "    model.extra_compute = False\n",
        "\n",
        "    model = model.to(device)\n",
        "\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.01)\n",
        "    # ^ One of many attempts of optimizing the training to get faster speeds\n",
        "    # as our systems weren't properly utilizing our GPUs for some reason.\n",
        "\n",
        "    trained_model = train_model(\n",
        "        model,\n",
        "        train_loader,\n",
        "        val_loader,\n",
        "        criterion,\n",
        "        optimizer,\n",
        "        num_epochs=20,\n",
        "        device=device,\n",
        "        patience=5,\n",
        "        checkpoint_path='ResNet34.pth'\n",
        "    )\n",
        "\n",
        "    # Model renamed in post for clarity\n",
        "    print(\"Model training complete and saved to ResNet32.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "eb292d3a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eb292d3a",
        "outputId": "29735e90-5a2f-437e-f582-a08c3c176b1e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initializing dataset from directory: Train\n",
            "Found 51 class folders: ['Onion', 'Cabbage', 'Pumpkin', 'Potato', 'Corn', 'Garlic', 'Dragon_fruit', 'Raddish', 'Amaranth', 'Bottle Gourd', 'Watermelon', 'Beetroot', 'Tomato', 'Eggplant', 'Paprika', 'Bell pepper', 'Bitter Gourd', 'Ridge Gourd', 'Jalepeno', 'Lemon', 'Cantaloupe', 'Strawberry', 'Mango', 'Spiny Gourd', 'Apple', 'Cauliflower', 'Okra', 'Kiwi', 'Banana', 'Blueberry', 'Sweetcorn', 'Pineapple', 'Peas', 'Grapes', 'Orange', 'Ginger', 'Sweetpotato', 'Turnip', 'Capsicum', 'Raspberry', 'Pomegranate', 'Soy beans', 'Fig', 'Spinach', 'Broccoli', 'Coconut', 'Cucumber', 'Pear', 'Carrot', 'Sponge Gourd', 'Chilli pepper']\n",
            "Dataset created with 3990 images\n",
            "Found 51 classes: Amaranth, Apple, Banana, Beetroot, Bell pepper, Bitter Gourd, Blueberry, Bottle Gourd, Broccoli, Cabbage, Cantaloupe, Capsicum, Carrot, Cauliflower, Chilli pepper, Coconut, Corn, Cucumber, Dragon_fruit, Eggplant, Fig, Garlic, Ginger, Grapes, Jalepeno, Kiwi, Lemon, Mango, Okra, Onion, Orange, Paprika, Pear, Peas, Pineapple, Pomegranate, Potato, Pumpkin, Raddish, Raspberry, Ridge Gourd, Soy beans, Spinach, Spiny Gourd, Sponge Gourd, Strawberry, Sweetcorn, Sweetpotato, Tomato, Turnip, Watermelon\n",
            "Initializing dataset from directory: val\n",
            "Found 28 class folders: ['Cabbage', 'Corn', 'Garlic', 'Amaranth', 'Bottle Gourd', 'Beetroot', 'Eggplant', 'Bell pepper', 'Bitter Gourd', 'Jalepeno', 'Lemon', 'Cantaloupe', 'Mango', 'Apple', 'Cauliflower', 'Kiwi', 'Banana', 'Blueberry', 'Grapes', 'Ginger', 'Capsicum', 'Fig', 'Broccoli', 'Coconut', 'Cucumber', 'Carrot', 'Dragon Fruit', 'Chilli pepper']\n",
            "Dataset created with 275 images\n",
            "Found 28 classes: Amaranth, Apple, Banana, Beetroot, Bell pepper, Bitter Gourd, Blueberry, Bottle Gourd, Broccoli, Cabbage, Cantaloupe, Capsicum, Carrot, Cauliflower, Chilli pepper, Coconut, Corn, Cucumber, Dragon Fruit, Eggplant, Fig, Garlic, Ginger, Grapes, Jalepeno, Kiwi, Lemon, Mango\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Downloading: \"https://download.pytorch.org/models/resnet34-b627a593.pth\" to /root/.cache/torch/hub/checkpoints/resnet34-b627a593.pth\n",
            "100%|██████████| 83.3M/83.3M [00:00<00:00, 186MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "VRAM Usage: 0.09 GB / 15.83 GB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/20 - Training:  10%|▉         | 12/125 [00:14<00:58,  1.93it/s]/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "Epoch 1/20 - Training:  10%|█         | 13/125 [00:14<00:53,  2.08it/s]/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "Epoch 1/20 - Training:  19%|█▉        | 24/125 [00:27<01:01,  1.65it/s]/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "Epoch 1/20 - Training:  32%|███▏      | 40/125 [00:41<00:39,  2.15it/s]/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "Epoch 1/20 - Training: 100%|██████████| 125/125 [02:05<00:00,  1.00s/it]\n",
            "Epoch 1/20 - Validation: 100%|██████████| 9/9 [00:09<00:00,  1.08s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20:\n",
            "  Train Loss: 3.5038, Val Loss: 3.2553\n",
            "  Val Accuracy: 0.0909\n",
            "  Saved new best model with accuracy: 0.0909\n",
            "VRAM Usage: 0.38 GB / 15.83 GB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rEpoch 2/20 - Training:   0%|          | 0/125 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "Epoch 2/20 - Training:   1%|          | 1/125 [00:06<14:12,  6.87s/it]/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "Epoch 2/20 - Training:  23%|██▎       | 29/125 [00:30<01:10,  1.37it/s]/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "Epoch 2/20 - Training: 100%|██████████| 125/125 [02:00<00:00,  1.03it/s]\n",
            "Epoch 2/20 - Validation: 100%|██████████| 9/9 [00:09<00:00,  1.05s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2/20:\n",
            "  Train Loss: 3.2356, Val Loss: 3.0563\n",
            "  Val Accuracy: 0.1636\n",
            "  Saved new best model with accuracy: 0.1636\n",
            "VRAM Usage: 0.38 GB / 15.83 GB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/20 - Training:   4%|▍         | 5/125 [00:08<03:21,  1.68s/it]/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "Epoch 3/20 - Training:   7%|▋         | 9/125 [00:12<02:06,  1.09s/it]/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "Epoch 3/20 - Training:  10%|█         | 13/125 [00:15<01:13,  1.52it/s]/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "Epoch 3/20 - Training:  85%|████████▍ | 106/125 [01:46<00:10,  1.79it/s]/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "Epoch 3/20 - Training: 100%|██████████| 125/125 [02:01<00:00,  1.03it/s]\n",
            "Epoch 3/20 - Validation: 100%|██████████| 9/9 [00:08<00:00,  1.00it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/20:\n",
            "  Train Loss: 3.1543, Val Loss: 2.9442\n",
            "  Val Accuracy: 0.1673\n",
            "  Saved new best model with accuracy: 0.1673\n",
            "VRAM Usage: 0.38 GB / 15.83 GB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rEpoch 4/20 - Training:   0%|          | 0/125 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "Epoch 4/20 - Training:   3%|▎         | 4/125 [00:07<02:04,  1.03s/it]/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "Epoch 4/20 - Training:  33%|███▎      | 41/125 [00:40<00:49,  1.71it/s]/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "Epoch 4/20 - Training:  39%|███▉      | 49/125 [00:49<00:35,  2.15it/s]/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "Epoch 4/20 - Training: 100%|██████████| 125/125 [02:00<00:00,  1.04it/s]\n",
            "Epoch 4/20 - Validation: 100%|██████████| 9/9 [00:08<00:00,  1.07it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4/20:\n",
            "  Train Loss: 3.1124, Val Loss: 2.9424\n",
            "  Val Accuracy: 0.1636\n",
            "  No improvement for 1 epochs. Best accuracy: 0.1673\n",
            "VRAM Usage: 0.38 GB / 15.83 GB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5/20 - Training:   4%|▍         | 5/125 [00:09<04:10,  2.09s/it]/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "Epoch 5/20 - Training:   6%|▋         | 8/125 [00:10<01:27,  1.33it/s]/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "Epoch 5/20 - Training:  16%|█▌        | 20/125 [00:22<01:00,  1.74it/s]/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "Epoch 5/20 - Training: 100%|██████████| 125/125 [02:01<00:00,  1.03it/s]\n",
            "Epoch 5/20 - Validation: 100%|██████████| 9/9 [00:08<00:00,  1.11it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5/20:\n",
            "  Train Loss: 2.9931, Val Loss: 3.0609\n",
            "  Val Accuracy: 0.1564\n",
            "  No improvement for 2 epochs. Best accuracy: 0.1673\n",
            "VRAM Usage: 0.38 GB / 15.83 GB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 6/20 - Training:   7%|▋         | 9/125 [00:11<01:21,  1.42it/s]/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "Epoch 6/20 - Training:  23%|██▎       | 29/125 [00:31<01:30,  1.06it/s]/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "Epoch 6/20 - Training:  45%|████▍     | 56/125 [01:02<01:52,  1.63s/it]/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "Epoch 6/20 - Training:  47%|████▋     | 59/125 [01:03<00:43,  1.50it/s]/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "Epoch 6/20 - Training: 100%|██████████| 125/125 [02:01<00:00,  1.03it/s]\n",
            "Epoch 6/20 - Validation: 100%|██████████| 9/9 [00:08<00:00,  1.11it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6/20:\n",
            "  Train Loss: 2.8951, Val Loss: 2.7499\n",
            "  Val Accuracy: 0.1527\n",
            "  No improvement for 3 epochs. Best accuracy: 0.1673\n",
            "VRAM Usage: 0.38 GB / 15.83 GB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 7/20 - Training:  14%|█▍        | 18/125 [00:21<01:19,  1.35it/s]/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "Epoch 7/20 - Training:  24%|██▍       | 30/125 [00:32<01:05,  1.46it/s]/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "Epoch 7/20 - Training:  28%|██▊       | 35/125 [00:38<01:31,  1.02s/it]/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "Epoch 7/20 - Training:  42%|████▏     | 52/125 [00:53<00:59,  1.23it/s]/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "Epoch 7/20 - Training: 100%|██████████| 125/125 [02:00<00:00,  1.04it/s]\n",
            "Epoch 7/20 - Validation: 100%|██████████| 9/9 [00:08<00:00,  1.10it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 7/20:\n",
            "  Train Loss: 2.6804, Val Loss: 2.0992\n",
            "  Val Accuracy: 0.3491\n",
            "  Saved new best model with accuracy: 0.3491\n",
            "VRAM Usage: 0.38 GB / 15.83 GB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 8/20 - Training:  10%|▉         | 12/125 [00:15<01:27,  1.30it/s]/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "Epoch 8/20 - Training:  31%|███       | 39/125 [00:38<00:49,  1.75it/s]/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "Epoch 8/20 - Training:  51%|█████     | 64/125 [01:04<00:28,  2.17it/s]/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "Epoch 8/20 - Training: 100%|██████████| 125/125 [01:59<00:00,  1.05it/s]\n",
            "Epoch 8/20 - Validation: 100%|██████████| 9/9 [00:08<00:00,  1.10it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 8/20:\n",
            "  Train Loss: 2.5985, Val Loss: 2.0667\n",
            "  Val Accuracy: 0.3855\n",
            "  Saved new best model with accuracy: 0.3855\n",
            "VRAM Usage: 0.38 GB / 15.83 GB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 9/20 - Training:   7%|▋         | 9/125 [00:10<01:29,  1.29it/s]/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "Epoch 9/20 - Training:  19%|█▉        | 24/125 [00:25<00:57,  1.77it/s]/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "Epoch 9/20 - Training:  46%|████▋     | 58/125 [01:01<00:50,  1.33it/s]/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "Epoch 9/20 - Training:  72%|███████▏  | 90/125 [01:32<00:41,  1.18s/it]/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "Epoch 9/20 - Training: 100%|██████████| 125/125 [02:00<00:00,  1.04it/s]\n",
            "Epoch 9/20 - Validation: 100%|██████████| 9/9 [00:09<00:00,  1.01s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 9/20:\n",
            "  Train Loss: 2.5092, Val Loss: 2.1522\n",
            "  Val Accuracy: 0.3273\n",
            "  No improvement for 1 epochs. Best accuracy: 0.3855\n",
            "VRAM Usage: 0.38 GB / 15.83 GB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 10/20 - Training:   3%|▎         | 4/125 [00:04<01:24,  1.44it/s]/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "Epoch 10/20 - Training:  30%|██▉       | 37/125 [00:36<00:52,  1.68it/s]/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "Epoch 10/20 - Training:  46%|████▌     | 57/125 [00:56<00:47,  1.42it/s]/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "Epoch 10/20 - Training: 100%|██████████| 125/125 [01:59<00:00,  1.05it/s]\n",
            "Epoch 10/20 - Validation: 100%|██████████| 9/9 [00:09<00:00,  1.01s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10/20:\n",
            "  Train Loss: 2.4429, Val Loss: 1.9390\n",
            "  Val Accuracy: 0.3891\n",
            "  Saved new best model with accuracy: 0.3891\n",
            "VRAM Usage: 0.38 GB / 15.83 GB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 11/20 - Training:   7%|▋         | 9/125 [00:09<01:15,  1.53it/s]/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "Epoch 11/20 - Training:  14%|█▎        | 17/125 [00:18<01:07,  1.59it/s]/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "Epoch 11/20 - Training:  50%|████▉     | 62/125 [01:00<00:39,  1.59it/s]/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "Epoch 11/20 - Training:  80%|████████  | 100/125 [01:36<00:21,  1.17it/s]/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "Epoch 11/20 - Training: 100%|██████████| 125/125 [01:59<00:00,  1.04it/s]\n",
            "Epoch 11/20 - Validation: 100%|██████████| 9/9 [00:10<00:00,  1.18s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 11/20:\n",
            "  Train Loss: 2.4278, Val Loss: 1.9377\n",
            "  Val Accuracy: 0.4364\n",
            "  Saved new best model with accuracy: 0.4364\n",
            "VRAM Usage: 0.38 GB / 15.83 GB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 12/20 - Training:  11%|█         | 14/125 [00:18<02:02,  1.10s/it]/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "Epoch 12/20 - Training:  32%|███▏      | 40/125 [00:42<01:03,  1.34it/s]/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "Epoch 12/20 - Training:  58%|█████▊    | 72/125 [01:11<00:34,  1.53it/s]/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "Epoch 12/20 - Training:  76%|███████▌  | 95/125 [01:33<00:18,  1.62it/s]/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "Epoch 12/20 - Training: 100%|██████████| 125/125 [01:59<00:00,  1.05it/s]\n",
            "Epoch 12/20 - Validation: 100%|██████████| 9/9 [00:09<00:00,  1.04s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 12/20:\n",
            "  Train Loss: 2.3644, Val Loss: 2.1493\n",
            "  Val Accuracy: 0.3345\n",
            "  No improvement for 1 epochs. Best accuracy: 0.4364\n",
            "VRAM Usage: 0.38 GB / 15.83 GB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 13/20 - Training:   7%|▋         | 9/125 [00:11<02:10,  1.13s/it]/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "Epoch 13/20 - Training:  10%|▉         | 12/125 [00:13<01:32,  1.22it/s]/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "Epoch 13/20 - Training:  16%|█▌        | 20/125 [00:24<01:50,  1.05s/it]/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "Epoch 13/20 - Training: 100%|██████████| 125/125 [01:58<00:00,  1.05it/s]\n",
            "Epoch 13/20 - Validation: 100%|██████████| 9/9 [00:09<00:00,  1.07s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 13/20:\n",
            "  Train Loss: 2.3025, Val Loss: 1.8926\n",
            "  Val Accuracy: 0.3491\n",
            "  No improvement for 2 epochs. Best accuracy: 0.4364\n",
            "VRAM Usage: 0.38 GB / 15.83 GB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 14/20 - Training:   3%|▎         | 4/125 [00:06<01:50,  1.09it/s]/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "Epoch 14/20 - Training:  18%|█▊        | 23/125 [00:25<01:01,  1.65it/s]/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "Epoch 14/20 - Training:  58%|█████▊    | 73/125 [01:14<00:35,  1.46it/s]/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "Epoch 14/20 - Training:  94%|█████████▍| 118/125 [01:57<00:05,  1.35it/s]/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "Epoch 14/20 - Training: 100%|██████████| 125/125 [02:02<00:00,  1.02it/s]\n",
            "Epoch 14/20 - Validation: 100%|██████████| 9/9 [00:09<00:00,  1.03s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 14/20:\n",
            "  Train Loss: 2.2883, Val Loss: 2.2344\n",
            "  Val Accuracy: 0.2764\n",
            "  No improvement for 3 epochs. Best accuracy: 0.4364\n",
            "VRAM Usage: 0.38 GB / 15.83 GB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 15/20 - Training:   6%|▋         | 8/125 [00:09<01:27,  1.34it/s]/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "Epoch 15/20 - Training:  10%|▉         | 12/125 [00:13<01:19,  1.41it/s]/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "Epoch 15/20 - Training:  13%|█▎        | 16/125 [00:18<01:50,  1.02s/it]/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "Epoch 15/20 - Training:  56%|█████▌    | 70/125 [01:11<01:24,  1.54s/it]/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "Epoch 15/20 - Training: 100%|██████████| 125/125 [01:59<00:00,  1.04it/s]\n",
            "Epoch 15/20 - Validation: 100%|██████████| 9/9 [00:09<00:00,  1.01s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 15/20:\n",
            "  Train Loss: 2.1080, Val Loss: 1.5461\n",
            "  Val Accuracy: 0.5273\n",
            "  Saved new best model with accuracy: 0.5273\n",
            "VRAM Usage: 0.38 GB / 15.83 GB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 16/20 - Training:   3%|▎         | 4/125 [00:05<01:33,  1.29it/s]/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "Epoch 16/20 - Training:  22%|██▏       | 28/125 [00:26<00:50,  1.91it/s]/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "Epoch 16/20 - Training:  50%|█████     | 63/125 [01:02<00:37,  1.67it/s]/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "Epoch 16/20 - Training:  73%|███████▎  | 91/125 [01:28<00:20,  1.69it/s]/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "Epoch 16/20 - Training: 100%|██████████| 125/125 [02:00<00:00,  1.04it/s]\n",
            "Epoch 16/20 - Validation: 100%|██████████| 9/9 [00:08<00:00,  1.11it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 16/20:\n",
            "  Train Loss: 2.0380, Val Loss: 1.5347\n",
            "  Val Accuracy: 0.4836\n",
            "  No improvement for 1 epochs. Best accuracy: 0.5273\n",
            "VRAM Usage: 0.38 GB / 15.83 GB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 17/20 - Training:  10%|▉         | 12/125 [00:11<01:31,  1.24it/s]/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "Epoch 17/20 - Training:  17%|█▋        | 21/125 [00:19<01:34,  1.10it/s]/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "Epoch 17/20 - Training:  26%|██▋       | 33/125 [00:30<01:02,  1.48it/s]/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "Epoch 17/20 - Training:  41%|████      | 51/125 [00:49<01:23,  1.13s/it]/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "Epoch 17/20 - Training: 100%|██████████| 125/125 [02:01<00:00,  1.03it/s]\n",
            "Epoch 17/20 - Validation: 100%|██████████| 9/9 [00:08<00:00,  1.10it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 17/20:\n",
            "  Train Loss: 2.0053, Val Loss: 1.5422\n",
            "  Val Accuracy: 0.5345\n",
            "  Saved new best model with accuracy: 0.5345\n",
            "VRAM Usage: 0.38 GB / 15.83 GB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 18/20 - Training:  10%|▉         | 12/125 [00:11<00:55,  2.03it/s]/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "Epoch 18/20 - Training:  23%|██▎       | 29/125 [00:30<01:09,  1.39it/s]/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "Epoch 18/20 - Training:  30%|██▉       | 37/125 [00:39<01:10,  1.25it/s]/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "Epoch 18/20 - Training:  86%|████████▌ | 107/125 [01:45<00:16,  1.11it/s]/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "Epoch 18/20 - Training: 100%|██████████| 125/125 [02:00<00:00,  1.04it/s]\n",
            "Epoch 18/20 - Validation: 100%|██████████| 9/9 [00:08<00:00,  1.03it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 18/20:\n",
            "  Train Loss: 1.9462, Val Loss: 1.4898\n",
            "  Val Accuracy: 0.6036\n",
            "  Saved new best model with accuracy: 0.6036\n",
            "VRAM Usage: 0.38 GB / 15.83 GB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rEpoch 19/20 - Training:   0%|          | 0/125 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "Epoch 19/20 - Training:  27%|██▋       | 34/125 [00:32<01:21,  1.11it/s]/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "Epoch 19/20 - Training:  38%|███▊      | 48/125 [00:46<00:54,  1.42it/s]/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "Epoch 19/20 - Training:  54%|█████▍    | 68/125 [01:04<00:37,  1.50it/s]/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "Epoch 19/20 - Training: 100%|██████████| 125/125 [01:59<00:00,  1.05it/s]\n",
            "Epoch 19/20 - Validation: 100%|██████████| 9/9 [00:09<00:00,  1.02s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 19/20:\n",
            "  Train Loss: 1.9381, Val Loss: 1.4375\n",
            "  Val Accuracy: 0.5709\n",
            "  No improvement for 1 epochs. Best accuracy: 0.6036\n",
            "VRAM Usage: 0.38 GB / 15.83 GB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 20/20 - Training:  16%|█▌        | 20/125 [00:20<01:05,  1.60it/s]/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "Epoch 20/20 - Training:  28%|██▊       | 35/125 [00:33<00:59,  1.52it/s]/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "Epoch 20/20 - Training:  38%|███▊      | 47/125 [00:42<00:41,  1.90it/s]/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "Epoch 20/20 - Training:  44%|████▍     | 55/125 [00:55<01:44,  1.49s/it]/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "Epoch 20/20 - Training: 100%|██████████| 125/125 [02:00<00:00,  1.04it/s]\n",
            "Epoch 20/20 - Validation: 100%|██████████| 9/9 [00:09<00:00,  1.02s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 20/20:\n",
            "  Train Loss: 1.8866, Val Loss: 1.4127\n",
            "  Val Accuracy: 0.5418\n",
            "  No improvement for 2 epochs. Best accuracy: 0.6036\n",
            "Model training complete and saved to fruit_veg_model.pth\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eb7c2418",
      "metadata": {},
      "source": [
        "For this model, we used this particular kaggle dataset:\n",
        "\n",
        "https://www.kaggle.com/datasets/sunnyagarwal427444/food-ingredient-dataset-51\n",
        "\n",
        "This was the first model we worked on, and it was a bit of a learning experience. Initially, we had a different dataset we were going to use:\n",
        "\n",
        "https://www.kaggle.com/datasets/pes12017000148/food-ingredients-and-recipe-dataset-with-images/data\n",
        "\n",
        "It wasn't until we got to training this model that we realized--if our goal is to have our model(s) identify, from an image of ingredients that a person uploaded (maybe they just bought from the grocery store or they grouped together what ingredients they already had), which ingredients are in that image and feed back recipes containing those ingredients--then this dataset wouldn't work at all. This dataset contains images of recipes themselves; we can't use that to train our model for ingredient recognition. \n",
        "\n",
        "It was a tad bit of a grave oversight, and so we ultimately landed on the former dataset. If you're familiar with ResNet, you'd know that it is an \"image classification\" network--and we need \"object detection\" models. Networks like ResNet take an image as input and output a single class prediction for the whole image. That doesn't give us what we need for ingredient detection. Nonetheless, mistakes are part of the experience, so we figure instead of just scrapping it we may as well keep it and document it.\n",
        "\n",
        "Output Metrics:\n",
        "\n",
        "Peak validation accuracy was obtained in Epoch 18/20 (60.36%); accuracy decreased after the subsequent final epochs, showing potential overfitting. \n",
        "\n",
        "This is probably because our training set has 3990 images across 51 classes, while our validation set only has 275 (~7%) images across 28 classes. \n",
        "\n",
        "I attempted retraining the model with these changes:\n",
        "\n",
        "* Creating a random split from the overall data set and creating train/validation subsets based off that (w/ torch .random_split()) to ensure all 51 classes are represented in both the training and validation splits\n",
        "\n",
        "* Increased dropout rate to reduce the model's tendency to memorize the training data and actually recognize patterns\n",
        "\n",
        "With that said, my ideas did not work as the peak accuracy for this version was 38.6%. Just under half of the first version. The train and validation loss values were fluctuating, unlike the first version, where it was consistently decreasing with each epoch too, likely meaning the model was not learning efficiently."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
