{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X-52E9bRkHMV"
      },
      "outputs": [],
      "source": [
        "!pip install ultralytics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "5QE033zYITjS"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import shutil\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from ultralytics import YOLO\n",
        "import torch\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import yaml\n",
        "import seaborn as sns\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oxCzw-TxKOXA"
      },
      "outputs": [],
      "source": [
        "!unzip recipe_ingredients.zip -d recipe_ingredients"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "PQ6JLJeZKpKp"
      },
      "outputs": [],
      "source": [
        "random.seed(42)\n",
        "\n",
        "dataset_path = 'recipe_ingredients/'\n",
        "data_yaml_path = os.path.join(dataset_path, 'data.yaml')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "FskjjhNEw1kX"
      },
      "outputs": [],
      "source": [
        "# A NEW FUNCTION I'M ADDING TO ANALYZE OUR DATASET AND CREATE A MORE BALANCED ONE AS OUR YOLOv8 MODELS\n",
        "# HAVE BEEN POTENTIALLY TRAINING ON AN IMBALANCED DATASET\n",
        "\n",
        "def analyze_dataset(dataset_path):\n",
        "    with open(os.path.join(dataset_path, 'data.yaml'), 'r') as f:\n",
        "        data_yaml = yaml.safe_load(f)\n",
        "        class_names = data_yaml['names']\n",
        "\n",
        "    train_dir = os.path.join(dataset_path, 'train', 'labels')\n",
        "    class_counts = Counter()\n",
        "\n",
        "    for filename in os.listdir(train_dir):\n",
        "        if filename.endswith('.txt'):\n",
        "            with open(os.path.join(train_dir, filename), 'r') as f:\n",
        "                for line in f:\n",
        "                    if line.strip():\n",
        "                        class_id = int(line.strip().split()[0])\n",
        "                        class_counts[class_id] += 1\n",
        "\n",
        "    plt.figure(figsize=(15, 8))\n",
        "    counts = [class_counts.get(i, 0) for i in range(len(class_names))]\n",
        "    class_labels = [f\"{class_names[i]} ({counts[i]})\" for i in range(len(class_names))]\n",
        "\n",
        "    sorted_indices = np.argsort(counts)\n",
        "    sorted_classes = [class_labels[i] for i in sorted_indices]\n",
        "    sorted_counts = [counts[i] for i in sorted_indices]\n",
        "\n",
        "    plt.barh(sorted_classes, sorted_counts)\n",
        "    plt.xlabel('Number of instances')\n",
        "    plt.title('Class Distribution in Training Data')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('class_distribution.png')\n",
        "\n",
        "    print(\"Class distribution:\")\n",
        "    for i, name in enumerate(class_names):\n",
        "        print(f\"{name}: {class_counts.get(i, 0)} instances\")\n",
        "\n",
        "    few_samples = [class_names[i] for i in range(len(class_names)) if class_counts.get(i, 0) < 100]\n",
        "    many_samples = [class_names[i] for i in range(len(class_names)) if class_counts.get(i, 0) > 1000]\n",
        "\n",
        "    return class_counts, few_samples, many_samples, class_names\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l8mZgxy6K05j"
      },
      "outputs": [],
      "source": [
        "def create_balanced_dataset(dataset_path, max_per_class=300, min_per_class=50):\n",
        "\n",
        "    with open(os.path.join(dataset_path, 'data.yaml'), 'r') as f:\n",
        "        data_yaml = yaml.safe_load(f)\n",
        "        class_names = data_yaml['names']\n",
        "\n",
        "    balanced_dir = os.path.join(dataset_path, 'train_balanced')\n",
        "    os.makedirs(os.path.join(balanced_dir, 'images'), exist_ok=True)\n",
        "    os.makedirs(os.path.join(balanced_dir, 'labels'), exist_ok=True)\n",
        "\n",
        "    class_counts, _, _, _ = analyze_dataset(dataset_path)\n",
        "\n",
        "    train_dir = os.path.join(dataset_path, 'train')\n",
        "    class_to_images = {i: [] for i in range(len(class_names))}\n",
        "\n",
        "    print(\"Mapping images to classes...\")\n",
        "    for label_file in os.listdir(os.path.join(train_dir, 'labels')):\n",
        "        if not label_file.endswith('.txt'):\n",
        "            continue\n",
        "\n",
        "        image_name = os.path.splitext(label_file)[0]\n",
        "        img_path = None\n",
        "\n",
        "        for ext in ['.jpg']:\n",
        "            test_path = os.path.join(train_dir, 'images', image_name + ext)\n",
        "            if os.path.exists(test_path):\n",
        "                img_path = test_path\n",
        "                break\n",
        "\n",
        "        if img_path is None:\n",
        "            continue\n",
        "\n",
        "        with open(os.path.join(train_dir, 'labels', label_file), 'r') as f:\n",
        "            image_classes = set()\n",
        "            for line in f:\n",
        "                if line.strip():\n",
        "                    class_id = int(line.strip().split()[0])\n",
        "                    image_classes.add(class_id)\n",
        "\n",
        "        for class_id in image_classes:\n",
        "            class_to_images[class_id].append((label_file, img_path))\n",
        "\n",
        "    selected_samples = set()\n",
        "\n",
        "    for class_id, images in class_to_images.items():\n",
        "        num_samples = len(images)\n",
        "        target_samples = max(min(num_samples, max_per_class), min_per_class)\n",
        "\n",
        "        if target_samples > num_samples:\n",
        "            samples_to_select = images\n",
        "        else:\n",
        "            samples_to_select = random.sample(images, target_samples)\n",
        "\n",
        "        for sample in samples_to_select:\n",
        "            selected_samples.add(sample)\n",
        "\n",
        "    print(f\"Creating balanced dataset with {len(selected_samples)} images\")\n",
        "\n",
        "    for label_file, img_path in selected_samples:\n",
        "        src_label = os.path.join(train_dir, 'labels', label_file)\n",
        "        dst_label = os.path.join(balanced_dir, 'labels', label_file)\n",
        "        shutil.copy(src_label, dst_label)\n",
        "\n",
        "        img_filename = os.path.basename(img_path)\n",
        "        dst_img = os.path.join(balanced_dir, 'images', img_filename)\n",
        "        shutil.copy(img_path, dst_img)\n",
        "\n",
        "    with open(os.path.join(dataset_path, 'data.yaml'), 'r') as f:\n",
        "        yaml_content = f.read()\n",
        "\n",
        "    abs_dataset_path = os.path.abspath(dataset_path)\n",
        "    abs_balanced_dir = os.path.join(abs_dataset_path, 'train_balanced')\n",
        "\n",
        "    yaml_content = f\"\"\"\n",
        "# YOLO dataset configuration file\n",
        "train: {abs_balanced_dir}/images\n",
        "val: {abs_dataset_path}/valid/images \n",
        "test: {abs_dataset_path}/test/images \n",
        "\n",
        "nc: {len(class_names)} \n",
        "names: {data_yaml['names']}\n",
        "\"\"\"\n",
        "\n",
        "    balanced_yaml_path = os.path.join(dataset_path, 'data_balanced.yaml')\n",
        "    with open(balanced_yaml_path, 'w') as f:\n",
        "        f.write(yaml_content)\n",
        "\n",
        "    return balanced_yaml_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "kGWJrcIOMp-V"
      },
      "outputs": [],
      "source": [
        "def fine_tune_model(model_path, yaml_path):\n",
        "    model = YOLO(model_path)\n",
        "\n",
        "    results = model.train(\n",
        "        data=yaml_path,\n",
        "        epochs=15,\n",
        "        patience=7,\n",
        "        batch=32,\n",
        "        imgsz=640,\n",
        "\n",
        "        augment=True,\n",
        "        degrees=15.0,\n",
        "        translate=0.2,\n",
        "        scale=0.5,\n",
        "        fliplr=0.5,\n",
        "        mosaic=1.0,\n",
        "        mixup=0.1,\n",
        "\n",
        "        lr0=0.0005,\n",
        "        lrf=0.01,\n",
        "\n",
        "        freeze=[0, 1, 2, 3, 4, 5]\n",
        "    )\n",
        "\n",
        "    model.export(format=\"pt\", name=\"finetuned_yolov8.pt\")\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "qTQffS1Z2YYx"
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "    dataset_path = \"recipe_ingredients/\"\n",
        "    existing_model_path = \"best.pt\"\n",
        "\n",
        "    class_counts, few_samples, many_samples, class_names = analyze_dataset(dataset_path)\n",
        "    print(f\"Classes with few samples: {few_samples}\")\n",
        "    print(f\"Classes with many samples: {many_samples}\")\n",
        "\n",
        "    balanced_yaml_path = create_balanced_dataset(dataset_path, max_per_class=300, min_per_class=50)\n",
        "\n",
        "    model = fine_tune_model(existing_model_path, balanced_yaml_path)\n",
        "\n",
        "    print(\"Fine-tuning and testing complete\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "-bJlQLl12eLL",
        "outputId": "0061e6c4-25af-4f0d-b521-32e0dc51956c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Class distribution:\n",
            "0: 14 instances\n",
            "1: 48 instances\n",
            "2: 3360 instances\n",
            "3: 316 instances\n",
            "4: 1506 instances\n",
            "5: 1178 instances\n",
            "6: 8190 instances\n",
            "7: 24 instances\n",
            "8: 632 instances\n",
            "9: 564 instances\n",
            "10: 516 instances\n",
            "11: 194 instances\n",
            "12: 120 instances\n",
            "13: 6894 instances\n",
            "14: 6532 instances\n",
            "15: 858 instances\n",
            "16: 80 instances\n",
            "17: 21898 instances\n",
            "18: 2784 instances\n",
            "19: 206 instances\n",
            "20: 226 instances\n",
            "21: 19148 instances\n",
            "22: 9354 instances\n",
            "23: 4978 instances\n",
            "24: 178 instances\n",
            "25: 126 instances\n",
            "26: 76 instances\n",
            "27: 108 instances\n",
            "28: 876 instances\n",
            "29: 422 instances\n",
            "30: 13562 instances\n",
            "31: 76 instances\n",
            "Classes with few samples: ['bay_leaves', 'beef', 'chickpeas', 'green_onion', 'salt', 'turmeric']\n",
            "Classes with many samples: ['bell_pepper', 'carrot', 'cauliflower', 'chicken', 'garlic', 'ginger', 'kumquat', 'lemon', 'onion', 'pork', 'potato', 'tomao']\n",
            "Class distribution:\n",
            "0: 14 instances\n",
            "1: 48 instances\n",
            "2: 3360 instances\n",
            "3: 316 instances\n",
            "4: 1506 instances\n",
            "5: 1178 instances\n",
            "6: 8190 instances\n",
            "7: 24 instances\n",
            "8: 632 instances\n",
            "9: 564 instances\n",
            "10: 516 instances\n",
            "11: 194 instances\n",
            "12: 120 instances\n",
            "13: 6894 instances\n",
            "14: 6532 instances\n",
            "15: 858 instances\n",
            "16: 80 instances\n",
            "17: 21898 instances\n",
            "18: 2784 instances\n",
            "19: 206 instances\n",
            "20: 226 instances\n",
            "21: 19148 instances\n",
            "22: 9354 instances\n",
            "23: 4978 instances\n",
            "24: 178 instances\n",
            "25: 126 instances\n",
            "26: 76 instances\n",
            "27: 108 instances\n",
            "28: 876 instances\n",
            "29: 422 instances\n",
            "30: 13562 instances\n",
            "31: 76 instances\n",
            "Mapping images to classes...\n",
            "Creating balanced dataset with 5690 images\n",
            "Ultralytics 8.3.116 ðŸš€ Python-3.11.12 torch-2.6.0+cu124 CUDA:0 (Tesla T4, 15095MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=best.pt, data=recipe_ingredients/data_balanced.yaml, epochs=15, time=None, patience=7, batch=32, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train2, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=[0, 1, 2, 3, 4, 5], multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=True, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.0005, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=15.0, translate=0.2, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.1, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train2\n",
            "Downloading https://ultralytics.com/assets/Arial.ttf to '/root/.config/Ultralytics/Arial.ttf'...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 755k/755k [00:00<00:00, 143MB/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       928  ultralytics.nn.modules.conv.Conv             [3, 32, 3, 2]                 \n",
            "  1                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
            "  2                  -1  1     29056  ultralytics.nn.modules.block.C2f             [64, 64, 1, True]             \n",
            "  3                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  4                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
            "  5                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  6                  -1  2    788480  ultralytics.nn.modules.block.C2f             [256, 256, 2, True]           \n",
            "  7                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n",
            "  8                  -1  1   1838080  ultralytics.nn.modules.block.C2f             [512, 512, 1, True]           \n",
            "  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 12                  -1  1    591360  ultralytics.nn.modules.block.C2f             [768, 256, 1]                 \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 15                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
            " 16                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 18                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
            " 19                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 21                  -1  1   1969152  ultralytics.nn.modules.block.C2f             [768, 512, 1]                 \n",
            " 22        [15, 18, 21]  1   2128432  ultralytics.nn.modules.head.Detect           [32, [128, 256, 512]]         \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model summary: 129 layers, 11,147,984 parameters, 11,147,968 gradients, 28.7 GFLOPs\n",
            "\n",
            "Transferred 355/355 items from pretrained weights\n",
            "Freezing layer 'model.0.conv.weight'\n",
            "Freezing layer 'model.0.bn.weight'\n",
            "Freezing layer 'model.0.bn.bias'\n",
            "Freezing layer 'model.1.conv.weight'\n",
            "Freezing layer 'model.1.bn.weight'\n",
            "Freezing layer 'model.1.bn.bias'\n",
            "Freezing layer 'model.2.cv1.conv.weight'\n",
            "Freezing layer 'model.2.cv1.bn.weight'\n",
            "Freezing layer 'model.2.cv1.bn.bias'\n",
            "Freezing layer 'model.2.cv2.conv.weight'\n",
            "Freezing layer 'model.2.cv2.bn.weight'\n",
            "Freezing layer 'model.2.cv2.bn.bias'\n",
            "Freezing layer 'model.2.m.0.cv1.conv.weight'\n",
            "Freezing layer 'model.2.m.0.cv1.bn.weight'\n",
            "Freezing layer 'model.2.m.0.cv1.bn.bias'\n",
            "Freezing layer 'model.2.m.0.cv2.conv.weight'\n",
            "Freezing layer 'model.2.m.0.cv2.bn.weight'\n",
            "Freezing layer 'model.2.m.0.cv2.bn.bias'\n",
            "Freezing layer 'model.3.conv.weight'\n",
            "Freezing layer 'model.3.bn.weight'\n",
            "Freezing layer 'model.3.bn.bias'\n",
            "Freezing layer 'model.4.cv1.conv.weight'\n",
            "Freezing layer 'model.4.cv1.bn.weight'\n",
            "Freezing layer 'model.4.cv1.bn.bias'\n",
            "Freezing layer 'model.4.cv2.conv.weight'\n",
            "Freezing layer 'model.4.cv2.bn.weight'\n",
            "Freezing layer 'model.4.cv2.bn.bias'\n",
            "Freezing layer 'model.4.m.0.cv1.conv.weight'\n",
            "Freezing layer 'model.4.m.0.cv1.bn.weight'\n",
            "Freezing layer 'model.4.m.0.cv1.bn.bias'\n",
            "Freezing layer 'model.4.m.0.cv2.conv.weight'\n",
            "Freezing layer 'model.4.m.0.cv2.bn.weight'\n",
            "Freezing layer 'model.4.m.0.cv2.bn.bias'\n",
            "Freezing layer 'model.4.m.1.cv1.conv.weight'\n",
            "Freezing layer 'model.4.m.1.cv1.bn.weight'\n",
            "Freezing layer 'model.4.m.1.cv1.bn.bias'\n",
            "Freezing layer 'model.4.m.1.cv2.conv.weight'\n",
            "Freezing layer 'model.4.m.1.cv2.bn.weight'\n",
            "Freezing layer 'model.4.m.1.cv2.bn.bias'\n",
            "Freezing layer 'model.5.conv.weight'\n",
            "Freezing layer 'model.5.bn.weight'\n",
            "Freezing layer 'model.5.bn.bias'\n",
            "Freezing layer 'model.22.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
            "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n.pt to 'yolo11n.pt'...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5.35M/5.35M [00:00<00:00, 301MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 1098.6Â±336.8 MB/s, size: 58.2 KB)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/recipe_ingredients/train_balanced/labels... 8005 images, 0 backgrounds, 1 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8005/8005 [00:04<00:00, 1619.85it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/recipe_ingredients/train_balanced/images/3255_jpg.rf.80dc6984b2ee3cf6b235129c1cbbade7.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0103]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/recipe_ingredients/train_balanced/labels.cache\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, num_output_channels=3, method='weighted_average'), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 434.9Â±233.0 MB/s, size: 43.0 KB)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/recipe_ingredients/valid/labels... 816 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 816/816 [00:01<00:00, 616.83it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/recipe_ingredients/valid/labels.cache\n",
            "Plotting labels to runs/detect/train2/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.0005' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000278, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
            "Image sizes 640 train, 640 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1mruns/detect/train2\u001b[0m\n",
            "Starting training for 15 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "       1/15       5.4G      1.245     0.8623      1.365         45        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 251/251 [03:23<00:00,  1.24it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:11<00:00,  1.13it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        816      10259      0.901      0.846      0.873      0.523\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "       2/15      6.95G       1.12     0.7855      1.292         56        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 251/251 [03:20<00:00,  1.25it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:10<00:00,  1.19it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        816      10259      0.862      0.825      0.859      0.575\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "       3/15      6.99G      1.105     0.7848      1.286         70        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 251/251 [03:16<00:00,  1.28it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:10<00:00,  1.22it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        816      10259      0.865      0.831      0.826      0.529\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "       4/15      7.01G      1.091     0.7803       1.28        103        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 251/251 [03:17<00:00,  1.27it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:10<00:00,  1.25it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        816      10259      0.898      0.852      0.876      0.589\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "       5/15      7.06G      1.072     0.7605       1.27         41        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 251/251 [03:15<00:00,  1.28it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:10<00:00,  1.22it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        816      10259      0.933      0.812      0.853       0.57\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Closing dataloader mosaic\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, num_output_channels=3, method='weighted_average'), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "       6/15      7.08G     0.9543     0.5177      1.215         56        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 251/251 [02:23<00:00,  1.75it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:09<00:00,  1.35it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        816      10259      0.838      0.868      0.868      0.604\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "       7/15      7.13G     0.9378     0.4961      1.201         27        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 251/251 [02:16<00:00,  1.84it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:10<00:00,  1.30it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        816      10259      0.864      0.848      0.889      0.616\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "       8/15      7.15G     0.9202     0.4783      1.187         20        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 251/251 [02:18<00:00,  1.81it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:09<00:00,  1.40it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        816      10259      0.872      0.866       0.91      0.629\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "       9/15       7.2G     0.9021     0.4642      1.176         40        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 251/251 [02:16<00:00,  1.84it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:09<00:00,  1.34it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        816      10259       0.91       0.87       0.88      0.634\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      10/15      7.22G     0.8837     0.4507      1.164         48        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 251/251 [02:18<00:00,  1.81it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:09<00:00,  1.44it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        816      10259      0.827      0.902      0.912      0.655\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      11/15      7.26G     0.8728     0.4399      1.156         24        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 251/251 [02:18<00:00,  1.82it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:09<00:00,  1.33it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        816      10259      0.866      0.874        0.9      0.651\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      12/15      7.29G     0.8511     0.4278      1.142         22        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 251/251 [02:19<00:00,  1.81it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:08<00:00,  1.46it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        816      10259      0.922      0.854      0.888      0.651\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      13/15      7.33G     0.8436     0.4222      1.138         19        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 251/251 [02:19<00:00,  1.80it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:09<00:00,  1.33it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        816      10259      0.901      0.873      0.907      0.661\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      14/15      7.36G     0.8244     0.4095      1.123         31        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 251/251 [02:19<00:00,  1.79it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:09<00:00,  1.38it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        816      10259       0.89      0.878      0.916      0.674\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      15/15       7.4G     0.8189     0.4009      1.118         51        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 251/251 [02:18<00:00,  1.82it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:09<00:00,  1.35it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        816      10259      0.896      0.882      0.916      0.677\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "15 epochs completed in 0.706 hours.\n",
            "Optimizer stripped from runs/detect/train2/weights/last.pt, 22.5MB\n",
            "Optimizer stripped from runs/detect/train2/weights/best.pt, 22.5MB\n",
            "\n",
            "Validating runs/detect/train2/weights/best.pt...\n",
            "Ultralytics 8.3.116 ðŸš€ Python-3.11.12 torch-2.6.0+cu124 CUDA:0 (Tesla T4, 15095MiB)\n",
            "Model summary (fused): 72 layers, 11,137,968 parameters, 0 gradients, 28.5 GFLOPs\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:21<00:00,  1.65s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        816      10259      0.916      0.875      0.925      0.674\n",
            "                  beef          2          4          1          0      0.537      0.404\n",
            "           bell_pepper        112        115      0.909      0.965      0.947      0.681\n",
            "               cabbage          1          1       0.65          1      0.995      0.895\n",
            "                carrot         83         92      0.932      0.946      0.957      0.661\n",
            "               chicken        121        435      0.959      0.957      0.972      0.771\n",
            "              cucumber         31         31      0.966      0.916      0.989      0.824\n",
            "              eggplant          7          7          1      0.953      0.995      0.808\n",
            "                garlic        302        510       0.95      0.976      0.982      0.684\n",
            "                ginger        417        765      0.964      0.992      0.972      0.834\n",
            "    green_chili_pepper         58         58      0.933      0.968      0.978      0.667\n",
            "           green_onion          4          4      0.768      0.841      0.945      0.688\n",
            "               kumquat        177       3131      0.993      0.999      0.993      0.732\n",
            "                 lemon         99        196      0.969      0.947       0.98      0.571\n",
            "                  okra          3          9      0.968      0.444       0.65      0.263\n",
            "                 onion        640       2209      0.989      0.979      0.989      0.776\n",
            "                  pork        207        824      0.861      0.835      0.892      0.533\n",
            "                potato        141        254      0.942      0.988      0.951      0.679\n",
            "               pumpkin          7          7      0.723      0.857      0.856      0.735\n",
            "          small_pepper         42         43       0.85       0.93      0.926      0.491\n",
            "                 tomao        327       1564      0.994      0.999      0.992       0.79\n",
            "Speed: 0.2ms preprocess, 12.7ms inference, 0.0ms loss, 1.9ms postprocess per image\n",
            "Results saved to \u001b[1mruns/detect/train2\u001b[0m\n"
          ]
        },
        {
          "ename": "SyntaxError",
          "evalue": "'\u001b[31m\u001b[1mfilename\u001b[0m' is not a valid YOLO argument. Similar arguments are i.e. ['name'].\n\n    Arguments received: ['yolo', '-f', '/root/.local/share/jupyter/runtime/kernel-06cb9951-e7d2-4877-ad09-0efd0f8473c2.json']. Ultralytics 'yolo' commands use the following syntax:\n\n        yolo TASK MODE ARGS\n\n        Where   TASK (optional) is one of frozenset({'obb', 'classify', 'segment', 'pose', 'detect'})\n                MODE (required) is one of frozenset({'predict', 'export', 'benchmark', 'track', 'train', 'val'})\n                ARGS (optional) are any number of custom 'arg=value' pairs like 'imgsz=320' that override defaults.\n                    See all ARGS at https://docs.ultralytics.com/usage/cfg or with 'yolo cfg'\n\n    1. Train a detection model for 10 epochs with an initial learning_rate of 0.01\n        yolo train data=coco8.yaml model=yolo11n.pt epochs=10 lr0=0.01\n\n    2. Predict a YouTube video using a pretrained segmentation model at image size 320:\n        yolo predict model=yolo11n-seg.pt source='https://youtu.be/LNwODJXcvt4' imgsz=320\n\n    3. Val a pretrained detection model at batch-size 1 and image size 640:\n        yolo val model=yolo11n.pt data=coco8.yaml batch=1 imgsz=640\n\n    4. Export a YOLO11n classification model to ONNX format at image size 224 by 128 (no TASK required)\n        yolo export model=yolo11n-cls.pt format=onnx imgsz=224,128\n\n    5. Ultralytics solutions usage\n        yolo solutions count or in ['crop', 'blur', 'workout', 'heatmap', 'isegment', 'visioneye', 'speed', 'queue', 'analytics', 'inference', 'trackzone'] source=\"path/to/video.mp4\"\n\n    6. Run special commands:\n        yolo help\n        yolo checks\n        yolo version\n        yolo settings\n        yolo copy-cfg\n        yolo cfg\n        yolo solutions help\n\n    Docs: https://docs.ultralytics.com\n    Solutions: https://docs.ultralytics.com/solutions/\n    Community: https://community.ultralytics.com\n    GitHub: https://github.com/ultralytics/ultralytics\n     (<string>)",
          "output_type": "error",
          "traceback": [
            "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
            "  File \u001b[1;32m\"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\"\u001b[0m, line \u001b[1;32m3553\u001b[0m, in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \u001b[1;32m\"<ipython-input-21-972361fa1b80>\"\u001b[0m, line \u001b[1;32m2\u001b[0m, in \u001b[1;35m<cell line: 0>\u001b[0m\n    main()\n",
            "  File \u001b[1;32m\"<ipython-input-17-b7371d2ce1a8>\"\u001b[0m, line \u001b[1;32m11\u001b[0m, in \u001b[1;35mmain\u001b[0m\n    model = fine_tune_model(existing_model_path, balanced_yaml_path)\n",
            "  File \u001b[1;32m\"<ipython-input-20-592528a50b92>\"\u001b[0m, line \u001b[1;32m25\u001b[0m, in \u001b[1;35mfine_tune_model\u001b[0m\n    model.export(format=\"pt\", filename=\"finetuned_yolov8.pt\")\n",
            "  File \u001b[1;32m\"/usr/local/lib/python3.11/dist-packages/ultralytics/engine/model.py\"\u001b[0m, line \u001b[1;32m727\u001b[0m, in \u001b[1;35mexport\u001b[0m\n    return Exporter(overrides=args, _callbacks=self.callbacks)(model=self.model)\n",
            "  File \u001b[1;32m\"/usr/local/lib/python3.11/dist-packages/ultralytics/engine/exporter.py\"\u001b[0m, line \u001b[1;32m240\u001b[0m, in \u001b[1;35m__init__\u001b[0m\n    self.args = get_cfg(cfg, overrides)\n",
            "  File \u001b[1;32m\"/usr/local/lib/python3.11/dist-packages/ultralytics/cfg/__init__.py\"\u001b[0m, line \u001b[1;32m309\u001b[0m, in \u001b[1;35mget_cfg\u001b[0m\n    check_dict_alignment(cfg, overrides)\n",
            "\u001b[0;36m  File \u001b[0;32m\"/usr/local/lib/python3.11/dist-packages/ultralytics/cfg/__init__.py\"\u001b[0;36m, line \u001b[0;32m499\u001b[0;36m, in \u001b[0;35mcheck_dict_alignment\u001b[0;36m\u001b[0m\n\u001b[0;31m    raise SyntaxError(string + CLI_HELP_MSG) from e\u001b[0m\n",
            "\u001b[0;36m  File \u001b[0;32m\"<string>\"\u001b[0;36m, line \u001b[0;32munknown\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m '\u001b[31m\u001b[1mfilename\u001b[0m' is not a valid YOLO argument. Similar arguments are i.e. ['name'].\n\n    Arguments received: ['yolo', '-f', '/root/.local/share/jupyter/runtime/kernel-06cb9951-e7d2-4877-ad09-0efd0f8473c2.json']. Ultralytics 'yolo' commands use the following syntax:\n\n        yolo TASK MODE ARGS\n\n        Where   TASK (optional) is one of frozenset({'obb', 'classify', 'segment', 'pose', 'detect'})\n                MODE (required) is one of frozenset({'predict', 'export', 'benchmark', 'track', 'train', 'val'})\n                ARGS (optional) are any number of custom 'arg=value' pairs like 'imgsz=320' that override defaults.\n                    See all ARGS at https://docs.ultralytics.com/usage/cfg or with 'yolo cfg'\n\n    1. Train a detection model for 10 epochs with an initial learning_rate of 0.01\n        yolo train data=coco8.yaml model=yolo11n.pt epochs=10 lr0=0.01\n\n    2. Predict a YouTube video using a pretrained segmentation model at image size 320:\n        yolo predict model=yolo11n-seg.pt source='https://youtu.be/LNwODJXcvt4' imgsz=320\n\n    3. Val a pretrained detection model at batch-size 1 and image size 640:\n        yolo val model=yolo11n.pt data=coco8.yaml batch=1 imgsz=640\n\n    4. Export a YOLO11n classification model to ONNX format at image size 224 by 128 (no TASK required)\n        yolo export model=yolo11n-cls.pt format=onnx imgsz=224,128\n\n    5. Ultralytics solutions usage\n        yolo solutions count or in ['crop', 'blur', 'workout', 'heatmap', 'isegment', 'visioneye', 'speed', 'queue', 'analytics', 'inference', 'trackzone'] source=\"path/to/video.mp4\"\n\n    6. Run special commands:\n        yolo help\n        yolo checks\n        yolo version\n        yolo settings\n        yolo copy-cfg\n        yolo cfg\n        yolo solutions help\n\n    Docs: https://docs.ultralytics.com\n    Solutions: https://docs.ultralytics.com/solutions/\n    Community: https://community.ultralytics.com\n    GitHub: https://github.com/ultralytics/ultralytics\n    \n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DNqfnF_VgTEa"
      },
      "source": [
        "We chose the YOLOv8 model because this particular dataset:\n",
        "\n",
        "https://universe.roboflow.com/food-w4zm1/recipe-ingredients-cn\n",
        "\n",
        "for the purposes of our app--identifying individual ingredients from an image of a group of ingredients--had exactly what we needed. An image of groups of ingredients, labeled with what ingredient they were and at what coordinates (boxed boundaries) they were located in the images.\n",
        "\n",
        "We chose the particular YOLOv8 model, because, it seemed to be the latest version that supported this particular dataset for our particular task (object detection).\n",
        "\n",
        "We spent a lot of time attempting to train our models on our own systems--and we managed to get it to recognize and \"use\" our GPUs, but we seemed to have run into bottleneck issues of sorts that failed to diagnose. Eventually, we used Google Colab, which, thankfully, does give us some limited usage with their Tesla 4 GPU, which trained our models quite fast.\n",
        "\n",
        "We'd only trained/validated/tested on a subset of the dataset:\n",
        "\n",
        "3957/15829 total images for training\n",
        "244/816 total images for validation\n",
        "221/1109 total images for testing\n",
        "\n",
        "Because of the limited GPU usage provided by Google Colab.\n",
        "\n",
        "We used image sizes of 640x640, because we didn't want them too small--as the model may have failed to properly recognize the images, and we didn't want them too large--as training may have taken too long.\n",
        "\n",
        "Ultralytic's .val() metrics returned 4 values:\n",
        "\n",
        "Box(P) -- Precision of bounding boxes; this measures the \"accuracy\" of the predicted boxes for the ingredients within an image = (0.878 final or 87.8%)\n",
        "\n",
        "R -- Recall; measures what % of objects are detected (0.879 final or 87.9%)\n",
        "\n",
        "mAP50 -- Mean Average Precision (at IoU threshold 0.5); quantifies the overall performance of our model (0.907 final or 90.7%)\n",
        "\n",
        "mAP50-95 -- Mean Average Precision avged over multiple IoU thresholds; more stringent/severe quantification of our model's performance (0.645 final or 64.5%)\n",
        "\n",
        "Class Performance:\n",
        "\n",
        "* High-performing classes:\n",
        "    * Cucumber: 0.995 mAP50 or 99.5%\n",
        "    * Tomato: 0.994 mAP50 or 99.4%\n",
        "    * Kumquat: 0.993 mAP50 or 99.3%\n",
        "    * Potato: 0.987 mAP50 or 98.7%\n",
        "* Low-performing classes:\n",
        "    * Green Onion: 0.376 mAP50 (only 3 instances in validation)\n",
        "\n",
        "We could train some more to address issues like green onion, but again, Google Colab only provides us a few uses and its \"reset schedule\" is unpredictable as it is undocumented."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
