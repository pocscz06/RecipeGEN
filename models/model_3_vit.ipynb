{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJu2DDktZzaz"
      },
      "source": [
        "libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "l9Fws4_OgI-G",
        "outputId": "b4a51619-5b77-4e46-9f46-b6a0466cd522"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: roboflow in /usr/local/lib/python3.11/dist-packages (1.1.61)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from roboflow) (2025.1.31)\n",
            "Requirement already satisfied: idna==3.7 in /usr/local/lib/python3.11/dist-packages (from roboflow) (3.7)\n",
            "Requirement already satisfied: cycler in /usr/local/lib/python3.11/dist-packages (from roboflow) (0.12.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from roboflow) (1.4.8)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from roboflow) (3.10.0)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from roboflow) (2.0.2)\n",
            "Requirement already satisfied: opencv-python-headless==4.10.0.84 in /usr/local/lib/python3.11/dist-packages (from roboflow) (4.10.0.84)\n",
            "Requirement already satisfied: Pillow>=7.1.2 in /usr/local/lib/python3.11/dist-packages (from roboflow) (11.1.0)\n",
            "Requirement already satisfied: pillow-heif>=0.18.0 in /usr/local/lib/python3.11/dist-packages (from roboflow) (0.22.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.11/dist-packages (from roboflow) (2.8.2)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.11/dist-packages (from roboflow) (1.1.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from roboflow) (2.32.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from roboflow) (1.17.0)\n",
            "Requirement already satisfied: urllib3>=1.26.6 in /usr/local/lib/python3.11/dist-packages (from roboflow) (2.3.0)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from roboflow) (4.67.1)\n",
            "Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from roboflow) (6.0.2)\n",
            "Requirement already satisfied: requests-toolbelt in /usr/local/lib/python3.11/dist-packages (from roboflow) (1.0.0)\n",
            "Requirement already satisfied: filetype in /usr/local/lib/python3.11/dist-packages (from roboflow) (1.2.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->roboflow) (1.3.2)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->roboflow) (4.57.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->roboflow) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->roboflow) (3.2.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->roboflow) (3.4.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: torch==2.6.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.6.0+cu124)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch==2.6.0->torchvision) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.6.0->torchvision) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install roboflow\n",
        "!pip install transformers\n",
        "!pip install torchvision"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sl6ALMQLYxKN"
      },
      "source": [
        "zip upload to g colab or manually pth upload"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "metadata": {
        "id": "0E8r13vqXyO_"
      },
      "outputs": [],
      "source": [
        "# from google.colab import files\n",
        "# uploaded_file = files.upload()\n",
        "# cancel when rerunning because another upload isn't needed when tuning // uncomment when needed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9o-bokkFteZN"
      },
      "source": [
        "trying api instead of preprocessing zip, not cleaned"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "BbDXRReZtfnI",
        "outputId": "e10e9a6f-664f-4b34-d275-fd52e6ffd7c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loading Roboflow workspace...\n",
            "loading Roboflow project...\n"
          ]
        }
      ],
      "source": [
        "from roboflow import Roboflow\n",
        "rf = Roboflow(api_key=\"ZSy2TaHq3naWbRd02t56\")\n",
        "project = rf.workspace(\"model3\").project(\"model3-2tttc\")\n",
        "version = project.version(2)\n",
        "dataset = version.download(\"folder\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFonuptVZ9Dt"
      },
      "source": [
        "imported libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {
        "id": "FpIo75nfXC2q"
      },
      "outputs": [],
      "source": [
        "from roboflow import Roboflow\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import ViTForImageClassification, ViTFeatureExtractor\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from google.colab import files\n",
        "import zipfile\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtqNcTTYZ4m6"
      },
      "source": [
        "unzip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "collapsed": true,
        "id": "y9Da32EMX-CX",
        "outputId": "61ec0a6a-4eb9-4f99-c58b-d2e11ea4da06"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nzip_path = \"Food Ingredient Recognition.v4i.multiclass.zip\"\\nextract_path = \"model3_dataset\"\\n\\nif not os.path.exists(extract_path):\\n    print(\"Extracting-----\")\\n    with zipfile.ZipFile(zip_path, \\'r\\') as zip_ref:\\n        zip_ref.extractall(extract_path)\\n    print(\"Extraction done\")\\nelse:\\n    print(\"data already extracted\")\\n'"
            ]
          },
          "execution_count": 142,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# robo flow \"Food Ingredient Recognition Computer Vision Project\" dataset\n",
        "\"\"\"\n",
        "zip_path = \"Food Ingredient Recognition.v4i.multiclass.zip\"\n",
        "extract_path = \"model3_dataset\"\n",
        "\n",
        "if not os.path.exists(extract_path):\n",
        "    print(\"Extracting-----\")\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_path)\n",
        "    print(\"Extraction done\")\n",
        "else:\n",
        "    print(\"data already extracted\")\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cL6vDxFZ_94s"
      },
      "source": [
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {
        "id": "T4ZyCM6rXNlI"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose([transforms.Resize((224, 224)),transforms.ToTensor()])\n",
        "\n",
        "try:\n",
        "    data_dir = dataset.location\n",
        "except NameError:\n",
        "    data_dir = \"model3_dataset\"\n",
        "\n",
        "train_dataset = datasets.ImageFolder(f\"{data_dir}/train\", transform=transform)\n",
        "valid_dataset = datasets.ImageFolder(f\"{data_dir}/valid\", transform=transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=16)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OGwsoKEC__V4"
      },
      "source": [
        "pth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9af49c0f",
        "outputId": "0c984cd5-c36c-44e6-84e0-22810820437e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized because the shapes did not match:\n",
            "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([577]) in the model instantiated\n",
            "- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([577, 768]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trained model loaded\n"
          ]
        }
      ],
      "source": [
        "NUM_CLASSES = len(train_dataset.classes)\n",
        "\n",
        "model = ViTForImageClassification.from_pretrained(\n",
        "    'google/vit-base-patch16-224',\n",
        "    num_labels=NUM_CLASSES,\n",
        "    ignore_mismatched_sizes=True\n",
        ")\n",
        "\n",
        "if os.path.exists(\"vit_model3.pth\"):\n",
        "    model.load_state_dict(torch.load(\"vit_model3.pth\"))\n",
        "    print(\"trained model loaded\")\n",
        "else:\n",
        "    print(\"vit_model3.pth not found\")\n",
        "    from google.colab import files\n",
        "    uploaded = files.upload()\n",
        "    model.load_state_dict(torch.load(\"vit_model3.pth\"))\n",
        "\n",
        "model = model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eRZ7D_itGEv"
      },
      "source": [
        "was drive, unneeded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {
        "collapsed": true,
        "id": "zlO5z0re1xyS"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7u6FOiJcdt9_"
      },
      "source": [
        "switch to CPU if GPU runs out of usage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {
        "id": "OOSPvuFvduMM"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HEujs8V1a3o4"
      },
      "source": [
        "vision transformer model , ViT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ijwlCUrKXVms",
        "outputId": "d50c1f28-249e-4667-d89e-f5522b5ab6dc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized because the shapes did not match:\n",
            "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([577]) in the model instantiated\n",
            "- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([577, 768]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "NUM_CLASSES = len(train_dataset.classes)\n",
        "model = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224', num_labels=NUM_CLASSES, ignore_mismatched_sizes=True)\n",
        "model.load_state_dict(torch.load(\"vit_model3.pth\"))\n",
        "# make sure runs gpu\n",
        "model = model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "metadata": {
        "id": "Byqf4JZRZIUo"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import numpy as np\n",
        "\n",
        "# training label\n",
        "all_labels = []\n",
        "for _, labels in train_loader:\n",
        "    all_labels.extend(labels.tolist())\n",
        "\n",
        "# classweigh t\n",
        "class_weights = compute_class_weight(\n",
        "    class_weight='balanced',\n",
        "    classes=np.unique(all_labels),\n",
        "    y=np.array(all_labels)\n",
        ")\n",
        "\n",
        "class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
        "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=3e-4)\n",
        "# make sure runs gpu\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iKHI8CRR_Opa"
      },
      "source": [
        "manual seed from best trial"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vqauFreL_OOU",
        "outputId": "d8ba840b-7f37-48a8-baa0-a6371f5b0894"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best (MANUAL) Seed: 42617\n"
          ]
        }
      ],
      "source": [
        "# fixed random seed for reproducibility\n",
        "import torch, numpy as np, random\n",
        "seed = 42617\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "print(f\"Best (MANUAL) Seed: {seed}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XkCKvMg0bBa3"
      },
      "source": [
        "training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "YmMEJL8lXaD1",
        "outputId": "ce4d9383-24bc-43d4-e993-763a085f0e54"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch #1, Batch 10 Loss: 6.2630\n",
            "Epoch #1, Batch 20 Loss: 5.9186\n",
            "Epoch #1, Batch 30 Loss: 5.6811\n",
            "Epoch #1, Batch 40 Loss: 5.8674\n",
            "Epoch #1, Batch 50 Loss: 6.8033\n",
            "Epoch #1, Batch 60 Loss: 5.6405\n",
            "Epoch #1, Batch 70 Loss: 6.2712\n",
            "Epoch #1, Batch 80 Loss: 7.0290\n",
            "Epoch #1, Batch 90 Loss: 6.5769\n",
            "Epoch #1, Batch 100 Loss: 7.1161\n",
            "Epoch #1, Batch 110 Loss: 6.3455\n",
            "Epoch #1, Batch 120 Loss: 6.0799\n",
            "Epoch #1, Batch 130 Loss: 5.7025\n",
            "Epoch #1, Batch 140 Loss: 7.0919\n",
            "Epoch #1, Batch 150 Loss: 6.1246\n",
            "Epoch #1, Batch 160 Loss: 5.9975\n",
            "Epoch #1, Batch 170 Loss: 6.3435\n",
            "Epoch #1, Batch 180 Loss: 5.9538\n",
            "Epoch #1, Batch 190 Loss: 5.9026\n",
            "Epoch #1, Batch 200 Loss: 6.8525\n",
            "Epoch #1, Batch 210 Loss: 6.3617\n",
            "Epoch #1, Batch 220 Loss: 6.2015\n",
            "Epoch #1, Batch 230 Loss: 6.3747\n",
            "Epoch #1, Batch 240 Loss: 6.6255\n",
            "Epoch #1, Batch 250 Loss: 6.1682\n",
            "Epoch #1, Batch 260 Loss: 5.7625\n",
            "Epoch #1, Batch 270 Loss: 6.1624\n",
            "Epoch #1, Batch 280 Loss: 5.8354\n",
            "Epoch #1, Batch 290 Loss: 5.5894\n",
            "Epoch #1, Batch 300 Loss: 5.9089\n",
            "Epoch #1, Batch 310 Loss: 6.0412\n",
            "Epoch #1, Batch 320 Loss: 7.1564\n",
            "Epoch #1, Batch 330 Loss: 6.1915\n",
            "Epoch #1, Batch 340 Loss: 6.0245\n",
            "Epoch #1, Batch 350 Loss: 6.2099\n",
            "Epoch #1, Batch 360 Loss: 6.0545\n",
            "Epoch #1, Batch 370 Loss: 6.0096\n",
            "Epoch #1, Batch 380 Loss: 5.7440\n",
            "Epoch #1, Batch 390 Loss: 7.2892\n",
            "Epoch #1, Batch 400 Loss: 5.8673\n",
            "Epoch #1, Batch 410 Loss: 6.2075\n",
            "Epoch #1, Batch 420 Loss: 5.5810\n",
            "Epoch #1, Batch 430 Loss: 5.8969\n",
            "Epoch #1, Batch 440 Loss: 5.5891\n",
            "Epoch #1, Batch 450 Loss: 5.9616\n",
            "Epoch #1, Batch 460 Loss: 5.7020\n",
            "Epoch #1, Batch 470 Loss: 6.7413\n",
            "Epoch #1, Batch 480 Loss: 6.4530\n",
            "Epoch #1, Batch 490 Loss: 6.4052\n",
            "Epoch #1, Batch 500 Loss: 6.6903\n",
            "Epoch #1, Batch 510 Loss: 5.9166\n",
            "Epoch #1, Batch 520 Loss: 5.8175\n",
            "Epoch #1, Batch 530 Loss: 5.9597\n",
            "Epoch #1, Batch 540 Loss: 5.8879\n",
            "Epoch #1, Batch 550 Loss: 5.5021\n",
            "Epoch #1, Batch 560 Loss: 6.0358\n",
            "Epoch #1, Batch 570 Loss: 6.7792\n",
            "Epoch #1, Batch 580 Loss: 6.0523\n",
            "Epoch #1, Batch 590 Loss: 6.4126\n",
            "Epoch #1, Batch 600 Loss: 6.7057\n",
            "Epoch #1, Batch 610 Loss: 5.7399\n",
            "Epoch #1, Batch 620 Loss: 6.2644\n",
            "Epoch #1, Batch 630 Loss: 6.8146\n",
            "Epoch #1, Batch 640 Loss: 6.0022\n",
            "Epoch #1, Batch 650 Loss: 6.1686\n",
            "Epoch #1, Batch 660 Loss: 5.5820\n",
            "Epoch #1, Batch 670 Loss: 5.2759\n",
            "Epoch #1, Batch 680 Loss: 6.4277\n",
            "Epoch #1, Batch 690 Loss: 6.3348\n",
            "Epoch #1, Batch 700 Loss: 6.1026\n",
            "Epoch #1, Batch 710 Loss: 7.0440\n",
            "Epoch #1 finished // Total loss: 4414.9838\n",
            "Epoch #2, Batch 10 Loss: 7.1015\n",
            "Epoch #2, Batch 20 Loss: 6.4974\n",
            "Epoch #2, Batch 30 Loss: 5.8871\n",
            "Epoch #2, Batch 40 Loss: 5.8575\n",
            "Epoch #2, Batch 50 Loss: 5.8105\n",
            "Epoch #2, Batch 60 Loss: 6.0504\n",
            "Epoch #2, Batch 70 Loss: 5.6875\n",
            "Epoch #2, Batch 80 Loss: 6.9088\n",
            "Epoch #2, Batch 90 Loss: 5.8045\n",
            "Epoch #2, Batch 100 Loss: 6.3675\n",
            "Epoch #2, Batch 110 Loss: 6.9051\n",
            "Epoch #2, Batch 120 Loss: 6.5587\n",
            "Epoch #2, Batch 130 Loss: 6.6945\n",
            "Epoch #2, Batch 140 Loss: 6.4224\n",
            "Epoch #2, Batch 150 Loss: 6.5505\n",
            "Epoch #2, Batch 160 Loss: 5.6026\n",
            "Epoch #2, Batch 170 Loss: 6.9512\n",
            "Epoch #2, Batch 180 Loss: 6.9818\n",
            "Epoch #2, Batch 190 Loss: 5.6181\n",
            "Epoch #2, Batch 200 Loss: 5.3181\n",
            "Epoch #2, Batch 210 Loss: 5.5656\n",
            "Epoch #2, Batch 220 Loss: 5.1647\n",
            "Epoch #2, Batch 230 Loss: 5.7757\n",
            "Epoch #2, Batch 240 Loss: 6.5783\n",
            "Epoch #2, Batch 250 Loss: 6.7881\n",
            "Epoch #2, Batch 260 Loss: 6.1306\n",
            "Epoch #2, Batch 270 Loss: 6.3190\n",
            "Epoch #2, Batch 280 Loss: 6.0619\n",
            "Epoch #2, Batch 290 Loss: 7.2920\n",
            "Epoch #2, Batch 300 Loss: 5.6879\n",
            "Epoch #2, Batch 310 Loss: 6.3693\n",
            "Epoch #2, Batch 320 Loss: 6.1840\n",
            "Epoch #2, Batch 330 Loss: 5.9291\n",
            "Epoch #2, Batch 340 Loss: 6.1012\n",
            "Epoch #2, Batch 350 Loss: 5.9563\n",
            "Epoch #2, Batch 360 Loss: 6.0740\n",
            "Epoch #2, Batch 370 Loss: 5.6475\n",
            "Epoch #2, Batch 380 Loss: 6.5278\n",
            "Epoch #2, Batch 390 Loss: 5.3376\n",
            "Epoch #2, Batch 400 Loss: 7.2379\n",
            "Epoch #2, Batch 410 Loss: 5.6284\n",
            "Epoch #2, Batch 420 Loss: 5.9733\n",
            "Epoch #2, Batch 430 Loss: 5.4581\n",
            "Epoch #2, Batch 440 Loss: 5.9150\n",
            "Epoch #2, Batch 450 Loss: 6.0961\n",
            "Epoch #2, Batch 460 Loss: 6.3923\n",
            "Epoch #2, Batch 470 Loss: 5.9448\n",
            "Epoch #2, Batch 480 Loss: 6.0179\n",
            "Epoch #2, Batch 490 Loss: 5.6924\n",
            "Epoch #2, Batch 500 Loss: 5.7089\n",
            "Epoch #2, Batch 510 Loss: 6.3324\n",
            "Epoch #2, Batch 520 Loss: 5.7956\n",
            "Epoch #2, Batch 530 Loss: 6.4171\n",
            "Epoch #2, Batch 540 Loss: 5.7284\n",
            "Epoch #2, Batch 550 Loss: 5.8068\n",
            "Epoch #2, Batch 560 Loss: 5.3637\n",
            "Epoch #2, Batch 570 Loss: 6.0186\n",
            "Epoch #2, Batch 580 Loss: 5.5749\n",
            "Epoch #2, Batch 590 Loss: 6.0536\n",
            "Epoch #2, Batch 600 Loss: 6.5578\n",
            "Epoch #2, Batch 610 Loss: 6.4548\n",
            "Epoch #2, Batch 620 Loss: 6.1619\n",
            "Epoch #2, Batch 630 Loss: 5.7948\n",
            "Epoch #2, Batch 640 Loss: 5.5594\n",
            "Epoch #2, Batch 650 Loss: 5.8786\n",
            "Epoch #2, Batch 660 Loss: 5.9440\n",
            "Epoch #2, Batch 670 Loss: 5.4058\n",
            "Epoch #2, Batch 680 Loss: 5.4494\n",
            "Epoch #2, Batch 690 Loss: 5.9236\n",
            "Epoch #2, Batch 700 Loss: 6.0150\n",
            "Epoch #2, Batch 710 Loss: 6.1831\n",
            "Epoch #2 finished // Total loss: 4327.2234\n",
            "Epoch #3, Batch 10 Loss: 6.5581\n",
            "Epoch #3, Batch 20 Loss: 5.7864\n",
            "Epoch #3, Batch 30 Loss: 6.7253\n",
            "Epoch #3, Batch 40 Loss: 5.8875\n",
            "Epoch #3, Batch 50 Loss: 6.2461\n",
            "Epoch #3, Batch 60 Loss: 6.8717\n",
            "Epoch #3, Batch 70 Loss: 6.2108\n",
            "Epoch #3, Batch 80 Loss: 5.8908\n",
            "Epoch #3, Batch 90 Loss: 5.8839\n",
            "Epoch #3, Batch 100 Loss: 5.4933\n",
            "Epoch #3, Batch 110 Loss: 6.2219\n",
            "Epoch #3, Batch 120 Loss: 5.9137\n",
            "Epoch #3, Batch 130 Loss: 5.9984\n",
            "Epoch #3, Batch 140 Loss: 6.0428\n",
            "Epoch #3, Batch 150 Loss: 5.7553\n",
            "Epoch #3, Batch 160 Loss: 5.4808\n",
            "Epoch #3, Batch 170 Loss: 6.3863\n",
            "Epoch #3, Batch 180 Loss: 6.1054\n",
            "Epoch #3, Batch 190 Loss: 5.6861\n",
            "Epoch #3, Batch 200 Loss: 6.6128\n",
            "Epoch #3, Batch 210 Loss: 6.2620\n",
            "Epoch #3, Batch 220 Loss: 5.5166\n",
            "Epoch #3, Batch 230 Loss: 5.8574\n",
            "Epoch #3, Batch 240 Loss: 6.2160\n",
            "Epoch #3, Batch 250 Loss: 5.5769\n",
            "Epoch #3, Batch 260 Loss: 6.3955\n",
            "Epoch #3, Batch 270 Loss: 5.9242\n",
            "Epoch #3, Batch 280 Loss: 6.5863\n",
            "Epoch #3, Batch 290 Loss: 6.2542\n",
            "Epoch #3, Batch 300 Loss: 5.9675\n",
            "Epoch #3, Batch 310 Loss: 6.7003\n",
            "Epoch #3, Batch 320 Loss: 5.5202\n",
            "Epoch #3, Batch 330 Loss: 6.7669\n",
            "Epoch #3, Batch 340 Loss: 5.3921\n",
            "Epoch #3, Batch 350 Loss: 6.2743\n",
            "Epoch #3, Batch 360 Loss: 5.7789\n",
            "Epoch #3, Batch 370 Loss: 5.8643\n",
            "Epoch #3, Batch 380 Loss: 6.2413\n",
            "Epoch #3, Batch 390 Loss: 6.2209\n",
            "Epoch #3, Batch 400 Loss: 5.9528\n",
            "Epoch #3, Batch 410 Loss: 5.8574\n",
            "Epoch #3, Batch 420 Loss: 5.5863\n",
            "Epoch #3, Batch 430 Loss: 6.0134\n",
            "Epoch #3, Batch 440 Loss: 6.2034\n",
            "Epoch #3, Batch 450 Loss: 5.5794\n",
            "Epoch #3, Batch 460 Loss: 5.8239\n",
            "Epoch #3, Batch 470 Loss: 5.6045\n",
            "Epoch #3, Batch 480 Loss: 5.8795\n",
            "Epoch #3, Batch 490 Loss: 5.5436\n",
            "Epoch #3, Batch 500 Loss: 6.0574\n",
            "Epoch #3, Batch 510 Loss: 6.2400\n",
            "Epoch #3, Batch 520 Loss: 5.7362\n",
            "Epoch #3, Batch 530 Loss: 6.0052\n",
            "Epoch #3, Batch 540 Loss: 5.8652\n",
            "Epoch #3, Batch 550 Loss: 6.1542\n",
            "Epoch #3, Batch 560 Loss: 5.6765\n",
            "Epoch #3, Batch 570 Loss: 5.7234\n",
            "Epoch #3, Batch 580 Loss: 5.7640\n",
            "Epoch #3, Batch 590 Loss: 5.5100\n",
            "Epoch #3, Batch 600 Loss: 5.8939\n",
            "Epoch #3, Batch 610 Loss: 5.9460\n",
            "Epoch #3, Batch 620 Loss: 6.7239\n",
            "Epoch #3, Batch 630 Loss: 5.6852\n",
            "Epoch #3, Batch 640 Loss: 5.5200\n",
            "Epoch #3, Batch 650 Loss: 6.2415\n",
            "Epoch #3, Batch 660 Loss: 5.6450\n",
            "Epoch #3, Batch 670 Loss: 5.9459\n",
            "Epoch #3, Batch 680 Loss: 5.8836\n",
            "Epoch #3, Batch 690 Loss: 6.6023\n",
            "Epoch #3, Batch 700 Loss: 5.5770\n",
            "Epoch #3, Batch 710 Loss: 5.5814\n",
            "Epoch #3 finished // Total loss: 4304.8848\n",
            "Epoch #4, Batch 10 Loss: 5.6584\n",
            "Epoch #4, Batch 20 Loss: 5.7950\n",
            "Epoch #4, Batch 30 Loss: 7.0651\n",
            "Epoch #4, Batch 40 Loss: 6.6217\n",
            "Epoch #4, Batch 50 Loss: 6.3936\n",
            "Epoch #4, Batch 60 Loss: 5.8157\n",
            "Epoch #4, Batch 70 Loss: 5.6539\n",
            "Epoch #4, Batch 80 Loss: 6.0027\n",
            "Epoch #4, Batch 90 Loss: 5.9348\n",
            "Epoch #4, Batch 100 Loss: 5.8890\n",
            "Epoch #4, Batch 110 Loss: 6.4405\n",
            "Epoch #4, Batch 120 Loss: 5.8814\n",
            "Epoch #4, Batch 130 Loss: 5.8612\n",
            "Epoch #4, Batch 140 Loss: 5.7563\n",
            "Epoch #4, Batch 150 Loss: 5.9955\n",
            "Epoch #4, Batch 160 Loss: 5.9710\n",
            "Epoch #4, Batch 170 Loss: 6.2551\n",
            "Epoch #4, Batch 180 Loss: 6.2251\n",
            "Epoch #4, Batch 190 Loss: 5.8437\n",
            "Epoch #4, Batch 200 Loss: 6.3948\n",
            "Epoch #4, Batch 210 Loss: 5.6646\n",
            "Epoch #4, Batch 220 Loss: 5.6150\n",
            "Epoch #4, Batch 230 Loss: 5.7230\n",
            "Epoch #4, Batch 240 Loss: 5.7615\n",
            "Epoch #4, Batch 250 Loss: 6.1298\n",
            "Epoch #4, Batch 260 Loss: 5.9801\n",
            "Epoch #4, Batch 270 Loss: 6.3347\n",
            "Epoch #4, Batch 280 Loss: 5.9691\n",
            "Epoch #4, Batch 290 Loss: 6.1180\n",
            "Epoch #4, Batch 300 Loss: 6.2259\n",
            "Epoch #4, Batch 310 Loss: 6.2189\n",
            "Epoch #4, Batch 320 Loss: 5.5408\n",
            "Epoch #4, Batch 330 Loss: 5.8261\n",
            "Epoch #4, Batch 340 Loss: 5.6403\n",
            "Epoch #4, Batch 350 Loss: 6.6566\n",
            "Epoch #4, Batch 360 Loss: 5.5515\n",
            "Epoch #4, Batch 370 Loss: 6.8839\n",
            "Epoch #4, Batch 380 Loss: 6.5049\n",
            "Epoch #4, Batch 390 Loss: 5.3722\n",
            "Epoch #4, Batch 400 Loss: 6.7304\n",
            "Epoch #4, Batch 410 Loss: 5.4851\n",
            "Epoch #4, Batch 420 Loss: 5.4089\n",
            "Epoch #4, Batch 430 Loss: 6.0679\n",
            "Epoch #4, Batch 440 Loss: 5.9173\n",
            "Epoch #4, Batch 450 Loss: 6.0047\n",
            "Epoch #4, Batch 460 Loss: 5.7225\n",
            "Epoch #4, Batch 470 Loss: 5.8303\n",
            "Epoch #4, Batch 480 Loss: 5.6199\n",
            "Epoch #4, Batch 490 Loss: 6.2076\n",
            "Epoch #4, Batch 500 Loss: 5.8249\n",
            "Epoch #4, Batch 510 Loss: 6.0626\n",
            "Epoch #4, Batch 520 Loss: 5.8745\n",
            "Epoch #4, Batch 530 Loss: 6.3255\n",
            "Epoch #4, Batch 540 Loss: 6.5858\n",
            "Epoch #4, Batch 550 Loss: 5.7046\n",
            "Epoch #4, Batch 560 Loss: 5.7767\n",
            "Epoch #4, Batch 570 Loss: 6.3491\n",
            "Epoch #4, Batch 580 Loss: 6.4385\n",
            "Epoch #4, Batch 590 Loss: 6.9363\n",
            "Epoch #4, Batch 600 Loss: 5.7818\n",
            "Epoch #4, Batch 610 Loss: 5.3272\n",
            "Epoch #4, Batch 620 Loss: 6.2621\n",
            "Epoch #4, Batch 630 Loss: 5.9061\n",
            "Epoch #4, Batch 640 Loss: 6.1250\n",
            "Epoch #4, Batch 650 Loss: 6.1050\n",
            "Epoch #4, Batch 660 Loss: 6.5942\n",
            "Epoch #4, Batch 670 Loss: 5.5854\n",
            "Epoch #4, Batch 680 Loss: 6.2195\n",
            "Epoch #4, Batch 690 Loss: 6.0270\n",
            "Epoch #4, Batch 700 Loss: 5.7725\n",
            "Epoch #4, Batch 710 Loss: 5.5848\n",
            "Epoch #4 finished // Total loss: 4289.6742\n",
            "Epoch #5, Batch 10 Loss: 5.9856\n",
            "Epoch #5, Batch 20 Loss: 5.9070\n",
            "Epoch #5, Batch 30 Loss: 6.0701\n",
            "Epoch #5, Batch 40 Loss: 6.8240\n",
            "Epoch #5, Batch 50 Loss: 6.0590\n",
            "Epoch #5, Batch 60 Loss: 6.1874\n",
            "Epoch #5, Batch 70 Loss: 5.0858\n",
            "Epoch #5, Batch 80 Loss: 5.9981\n",
            "Epoch #5, Batch 90 Loss: 5.3638\n",
            "Epoch #5, Batch 100 Loss: 6.1874\n",
            "Epoch #5, Batch 110 Loss: 5.5847\n",
            "Epoch #5, Batch 120 Loss: 5.8902\n",
            "Epoch #5, Batch 130 Loss: 5.8638\n",
            "Epoch #5, Batch 140 Loss: 6.1014\n",
            "Epoch #5, Batch 150 Loss: 5.6515\n",
            "Epoch #5, Batch 160 Loss: 5.7199\n",
            "Epoch #5, Batch 170 Loss: 5.8458\n",
            "Epoch #5, Batch 180 Loss: 6.0388\n",
            "Epoch #5, Batch 190 Loss: 6.0607\n",
            "Epoch #5, Batch 200 Loss: 5.7467\n",
            "Epoch #5, Batch 210 Loss: 7.1534\n",
            "Epoch #5, Batch 220 Loss: 6.6720\n",
            "Epoch #5, Batch 230 Loss: 5.7737\n",
            "Epoch #5, Batch 240 Loss: 6.4099\n",
            "Epoch #5, Batch 250 Loss: 6.5878\n",
            "Epoch #5, Batch 260 Loss: 6.5651\n",
            "Epoch #5, Batch 270 Loss: 5.8678\n",
            "Epoch #5, Batch 280 Loss: 5.6084\n",
            "Epoch #5, Batch 290 Loss: 5.9108\n",
            "Epoch #5, Batch 300 Loss: 5.7370\n",
            "Epoch #5, Batch 310 Loss: 5.5340\n",
            "Epoch #5, Batch 320 Loss: 5.7386\n",
            "Epoch #5, Batch 330 Loss: 6.3136\n",
            "Epoch #5, Batch 340 Loss: 5.7969\n",
            "Epoch #5, Batch 350 Loss: 6.6116\n",
            "Epoch #5, Batch 360 Loss: 6.1355\n",
            "Epoch #5, Batch 370 Loss: 6.6607\n",
            "Epoch #5, Batch 380 Loss: 5.8232\n",
            "Epoch #5, Batch 390 Loss: 6.7194\n",
            "Epoch #5, Batch 400 Loss: 5.6855\n",
            "Epoch #5, Batch 410 Loss: 5.7965\n",
            "Epoch #5, Batch 420 Loss: 5.8764\n",
            "Epoch #5, Batch 430 Loss: 5.1478\n",
            "Epoch #5, Batch 440 Loss: 5.9279\n",
            "Epoch #5, Batch 450 Loss: 6.6957\n",
            "Epoch #5, Batch 460 Loss: 6.2572\n",
            "Epoch #5, Batch 470 Loss: 5.6224\n",
            "Epoch #5, Batch 480 Loss: 5.8239\n",
            "Epoch #5, Batch 490 Loss: 6.5719\n",
            "Epoch #5, Batch 500 Loss: 5.9071\n",
            "Epoch #5, Batch 510 Loss: 6.5402\n",
            "Epoch #5, Batch 520 Loss: 5.7264\n",
            "Epoch #5, Batch 530 Loss: 5.8633\n",
            "Epoch #5, Batch 540 Loss: 6.7867\n",
            "Epoch #5, Batch 550 Loss: 6.6740\n",
            "Epoch #5, Batch 560 Loss: 5.4119\n",
            "Epoch #5, Batch 570 Loss: 5.8868\n",
            "Epoch #5, Batch 580 Loss: 5.6470\n",
            "Epoch #5, Batch 590 Loss: 5.9994\n",
            "Epoch #5, Batch 600 Loss: 5.9769\n",
            "Epoch #5, Batch 610 Loss: 5.3465\n",
            "Epoch #5, Batch 620 Loss: 6.2013\n",
            "Epoch #5, Batch 630 Loss: 5.4160\n",
            "Epoch #5, Batch 640 Loss: 5.8164\n",
            "Epoch #5, Batch 650 Loss: 5.6189\n",
            "Epoch #5, Batch 660 Loss: 5.9937\n",
            "Epoch #5, Batch 670 Loss: 6.2986\n",
            "Epoch #5, Batch 680 Loss: 5.9808\n",
            "Epoch #5, Batch 690 Loss: 5.8374\n",
            "Epoch #5, Batch 700 Loss: 5.4747\n",
            "Epoch #5, Batch 710 Loss: 5.9697\n",
            "Epoch #5 finished // Total loss: 4275.9163\n",
            "Epoch #6, Batch 10 Loss: 5.7250\n",
            "Epoch #6, Batch 20 Loss: 6.4794\n",
            "Epoch #6, Batch 30 Loss: 5.7027\n",
            "Epoch #6, Batch 40 Loss: 5.9547\n",
            "Epoch #6, Batch 50 Loss: 6.2215\n",
            "Epoch #6, Batch 60 Loss: 6.3556\n",
            "Epoch #6, Batch 70 Loss: 6.6511\n",
            "Epoch #6, Batch 80 Loss: 5.7073\n",
            "Epoch #6, Batch 90 Loss: 5.3919\n",
            "Epoch #6, Batch 100 Loss: 6.1765\n",
            "Epoch #6, Batch 110 Loss: 5.4020\n",
            "Epoch #6, Batch 120 Loss: 5.7688\n",
            "Epoch #6, Batch 130 Loss: 6.5607\n",
            "Epoch #6, Batch 140 Loss: 5.6900\n",
            "Epoch #6, Batch 150 Loss: 6.2635\n",
            "Epoch #6, Batch 160 Loss: 6.8008\n",
            "Epoch #6, Batch 170 Loss: 5.6374\n",
            "Epoch #6, Batch 180 Loss: 5.6562\n",
            "Epoch #6, Batch 190 Loss: 6.2347\n",
            "Epoch #6, Batch 200 Loss: 5.5226\n",
            "Epoch #6, Batch 210 Loss: 5.5736\n",
            "Epoch #6, Batch 220 Loss: 5.4770\n",
            "Epoch #6, Batch 230 Loss: 6.6867\n",
            "Epoch #6, Batch 240 Loss: 5.4769\n",
            "Epoch #6, Batch 250 Loss: 5.3304\n",
            "Epoch #6, Batch 260 Loss: 5.6794\n",
            "Epoch #6, Batch 270 Loss: 5.5275\n",
            "Epoch #6, Batch 280 Loss: 5.7762\n",
            "Epoch #6, Batch 290 Loss: 5.5864\n",
            "Epoch #6, Batch 300 Loss: 5.7764\n",
            "Epoch #6, Batch 310 Loss: 6.5684\n",
            "Epoch #6, Batch 320 Loss: 5.5136\n",
            "Epoch #6, Batch 330 Loss: 6.4525\n",
            "Epoch #6, Batch 340 Loss: 5.7265\n",
            "Epoch #6, Batch 350 Loss: 6.3731\n",
            "Epoch #6, Batch 360 Loss: 5.5307\n",
            "Epoch #6, Batch 370 Loss: 6.9053\n",
            "Epoch #6, Batch 380 Loss: 5.8921\n",
            "Epoch #6, Batch 390 Loss: 5.6459\n",
            "Epoch #6, Batch 400 Loss: 5.7064\n",
            "Epoch #6, Batch 410 Loss: 5.8194\n",
            "Epoch #6, Batch 420 Loss: 6.6318\n",
            "Epoch #6, Batch 430 Loss: 5.2539\n",
            "Epoch #6, Batch 440 Loss: 6.6862\n",
            "Epoch #6, Batch 450 Loss: 6.5741\n",
            "Epoch #6, Batch 460 Loss: 5.9834\n",
            "Epoch #6, Batch 470 Loss: 5.9602\n",
            "Epoch #6, Batch 480 Loss: 6.2473\n",
            "Epoch #6, Batch 490 Loss: 5.8058\n",
            "Epoch #6, Batch 500 Loss: 5.7985\n",
            "Epoch #6, Batch 510 Loss: 5.8311\n",
            "Epoch #6, Batch 520 Loss: 5.6747\n",
            "Epoch #6, Batch 530 Loss: 5.6286\n",
            "Epoch #6, Batch 540 Loss: 6.7891\n",
            "Epoch #6, Batch 550 Loss: 6.0225\n",
            "Epoch #6, Batch 560 Loss: 5.7493\n",
            "Epoch #6, Batch 570 Loss: 5.8437\n",
            "Epoch #6, Batch 580 Loss: 5.6117\n",
            "Epoch #6, Batch 590 Loss: 5.8803\n",
            "Epoch #6, Batch 600 Loss: 5.4824\n",
            "Epoch #6, Batch 610 Loss: 6.1230\n",
            "Epoch #6, Batch 620 Loss: 5.1073\n",
            "Epoch #6, Batch 630 Loss: 6.8647\n",
            "Epoch #6, Batch 640 Loss: 5.9537\n",
            "Epoch #6, Batch 650 Loss: 6.7134\n",
            "Epoch #6, Batch 660 Loss: 5.7013\n",
            "Epoch #6, Batch 670 Loss: 6.0568\n",
            "Epoch #6, Batch 680 Loss: 6.6329\n",
            "Epoch #6, Batch 690 Loss: 5.8743\n",
            "Epoch #6, Batch 700 Loss: 5.7391\n",
            "Epoch #6, Batch 710 Loss: 5.7495\n",
            "Epoch #6 finished // Total loss: 4255.9959\n",
            "Trial saved as vit_model3_trial.pth\n"
          ]
        }
      ],
      "source": [
        "# finding random seed until good one\n",
        "\"\"\"\n",
        "# random seed\n",
        "import torch, numpy as np, random\n",
        "seed = random.randint(0, 99999)\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "\n",
        "print(f\"Reinitialized seed: {seed}\")\n",
        "\n",
        "# reinitialize model\n",
        "NUM_CLASSES = len(train_dataset.classes)\n",
        "model = ViTForImageClassification.from_pretrained(\n",
        "    'google/vit-base-patch16-224',\n",
        "    num_labels=NUM_CLASSES,\n",
        "    ignore_mismatched_sizes=True\n",
        ")\n",
        "\"\"\"\n",
        "model = model.to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
        "\n",
        "# # of epochs\n",
        "for epoch in range(6):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch_idx, (imgs, labels) in enumerate(train_loader):\n",
        "        imgs, labels = imgs.to(device), labels.to(device)\n",
        "        outputs = model(imgs).logits\n",
        "        loss = criterion(outputs, labels)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        if (batch_idx + 1) % 10 == 0:\n",
        "            print(f\"Epoch #{epoch+1}, Batch {batch_idx+1} Loss: {loss.item():.4f}\")\n",
        "\n",
        "    print(f\"Epoch #{epoch+1} finished // Total loss: {total_loss:.4f}\")\n",
        "\n",
        "# trial save if good\n",
        "torch.save(model.state_dict(), \"vit_model3_trial.pth\")\n",
        "print(\"Trial saved as vit_model3_trial.pth\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L9SCraC4esxr"
      },
      "source": [
        "model save for safety in case g colab runs out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 151,
      "metadata": {
        "id": "CV7W02Z9et6q"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), \"vit_model3.pth\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Itpy9o8bEwq"
      },
      "source": [
        "model load and save"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 152,
      "metadata": {
        "id": "QqfQ0jl8ZFwM"
      },
      "outputs": [],
      "source": [
        "model.load_state_dict(torch.load(\"vit_model3.pth\"))\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AbaWiz0bbG96"
      },
      "source": [
        "model eval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 153,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2o73drYFXf6S",
        "outputId": "d274904a-3822-4fad-d2ac-fe990681eb1c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.000000%\n"
          ]
        }
      ],
      "source": [
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for imgs, labels in valid_loader:\n",
        "        imgs, labels = imgs.to(device), labels.to(device)\n",
        "        outputs = model(imgs).logits\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "accuracy = correct / total\n",
        "print(f\"Accuracy: {accuracy:.6%}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z150c49VR1KO"
      },
      "source": [
        "safety dwnload 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 154,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "Z38OsSVhRyzt",
        "outputId": "7c1d4aa0-b4d3-4682-94b9-93c76fa6f53f"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_36ce4954-eb00-460c-9652-6e082eb17656\", \"vit_model3_epoch3.pth\", 345051610)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from google.colab import files\n",
        "files.download(\"vit_model3_epoch3.pth\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZH5m6jZeE3r"
      },
      "source": [
        "safety dwnload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 155,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "f27e26a6",
        "outputId": "fb84e67a-b59e-4d61-9567-d43cb8dc275a"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_5f6c481a-1cc1-49ae-8043-cbd00988f1fa\", \"vit_model3.pth\", 345050182)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "\n",
        "from google.colab import files\n",
        "files.download(\"vit_model3.pth\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8ZRvPuF1Y6n"
      },
      "source": [
        "actual vs pred lables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 156,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dixrbuJsj0Pb",
        "outputId": "5b4a44fb-b204-454a-94a0-bedadae2f610"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predictions: tensor([320, 320, 382, 320, 320, 320, 320, 320, 320, 320], device='cuda:0')\n",
            "Actual labels: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1], device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "imgs, labels = next(iter(valid_loader))\n",
        "imgs, labels = imgs.to(device), labels.to(device)\n",
        "outputs = model(imgs).logits\n",
        "_, preds = torch.max(outputs, 1)\n",
        "\n",
        "print(\"Predictions:\", preds[:10])\n",
        "print(\"Actual labels:\", labels[:10])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6XJ1-VSi1U2w"
      },
      "source": [
        "prediction checker"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 157,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Go__TH4H1Tru",
        "outputId": "ebdc6229-cc05-4747-c0cf-383bc9f973cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Counter({np.int64(320): 369, np.int64(158): 111, np.int64(533): 96, np.int64(212): 92, np.int64(471): 17, np.int64(452): 9, np.int64(280): 7, np.int64(382): 2, np.int64(29): 2, np.int64(541): 1, np.int64(144): 1})\n"
          ]
        }
      ],
      "source": [
        "from collections import Counter\n",
        "all_preds = []\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for imgs, _ in valid_loader:\n",
        "        imgs = imgs.to(device)\n",
        "        outputs = model(imgs).logits\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "\n",
        "print(Counter(all_preds))\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
